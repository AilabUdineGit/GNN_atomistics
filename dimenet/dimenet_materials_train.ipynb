{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dimenet-materials-train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "M0TTCoXzowQi",
        "DG8nOP-zpD7X",
        "9TVsIsgvpJ_e",
        "gCUsRCZ6pUp0",
        "n1Gl2SWgpfqa",
        "plHW4EIJpsX7",
        "DQvcjWezqZof",
        "wSXWWjl0qqIf",
        "APtIHa1UrGEx"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeZyCbkfkuXx"
      },
      "source": [
        "# Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCADQ_Y-5AZ2"
      },
      "source": [
        "*   Make sure you run the model using a GPU (On Google Colab: Runtime -> Change Runtime Type -> GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNXgZoCHklOe"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "# install packages\n",
        "!pip install -q torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git\n",
        "!pip install -q torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q ase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI7ky8iYk18J"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "import numpy as np\n",
        "import torch\n",
        "import ase\n",
        "import random\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# set random seeds\n",
        "seed = 55555\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "# if you are using GPU\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zgbvaqek4Kb"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/FeMaterials/'\n",
        "TRAINING_RATIO = 0.8\n",
        "NUCLEAR_CHARGE = 26 # default nuclear charge\n",
        "OPTIMIZER = torch.optim.Adam\n",
        "DTYPE = torch.float64\n",
        "\n",
        "BATCH_SIZE = 6\n",
        "CRITERION = torch.nn.MSELoss()\n",
        "CROSSENTROPY = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def print_hyperparameters():\n",
        "  print(\"Default nuclear charge:\", NUCLEAR_CHARGE)\n",
        "  print(\"Training ratio:\", TRAINING_RATIO)\n",
        "  print(\"Batch size:\", BATCH_SIZE)\n",
        "  print(\"Optimizer:\", OPTIMIZER)\n",
        "  print(\"Learning rate:\", LEARNING_RATE)\n",
        "  print(\"Criterion:\", CRITERION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjbYYGWrn5Xf"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "import time\n",
        "from datetime import datetime\n",
        "import os\n",
        "from numpy import savetxt\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path2data = BASE_PATH + 'input/'\n",
        "\n",
        "# Create results directories\n",
        "now = datetime.now()\n",
        "resdir = BASE_PATH + 'dimenet-results/' + now.strftime(\"%Y%m%d-%H%M%S\") + '/'\n",
        "os.makedirs(resdir)\n",
        "res_models_dir = resdir + \"models/\";\n",
        "res_maes_dir = resdir + \"maes/\";\n",
        "res_graphs_dir = resdir + \"graphs/\";\n",
        "os.makedirs(res_models_dir);\n",
        "os.makedirs(res_maes_dir);\n",
        "os.makedirs(res_graphs_dir);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_aZDovaoq7J"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0TTCoXzowQi"
      },
      "source": [
        "## Pre-Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dum8n-V9oyw_"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "M_1_54 = None\n",
        "\n",
        "def extend_atoms(atoms: ase.Atoms, target: int):\n",
        "  global M_1_54\n",
        "  tmp = None\n",
        "  if M_1_54 is None or target != 54:\n",
        "    tmp = ase.build.find_optimal_cell_shape(atoms.get_cell(), target, \"sc\") \n",
        "    if target == 54:\n",
        "      M_1_54 = tmp\n",
        "  else:\n",
        "    tmp = M_1_54\n",
        "  supercell = ase.build.make_supercell(atoms, tmp)\n",
        "  supercell.info[\"energy\"] = atoms.info[\"energy\"] * int(target / atoms.get_global_number_of_atoms())\n",
        "  return supercell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJNeHV88o0TT"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def data_object(atoms: ase.Atoms, num_atoms=54):\n",
        "  \n",
        "  n = atoms.get_global_number_of_atoms()\n",
        "\n",
        "  if n == 1: # one atom structure - need to expand\n",
        "    atoms = extend_atoms(atoms, num_atoms)\n",
        "\n",
        "  n = atoms.get_global_number_of_atoms()\n",
        "\n",
        "  cell = torch.tensor(atoms.cell, dtype=DTYPE).to(device)\n",
        "  positions = torch.tensor(atoms.get_positions(), dtype=DTYPE).to(device)\n",
        "  \n",
        "  charges = [ NUCLEAR_CHARGE ] * len(positions)\n",
        "  charges = torch.tensor(charges, dtype=torch.long).to(device)\n",
        "\n",
        "  y = torch.tensor(atoms.info[\"energy\"], dtype=DTYPE).to(device)\n",
        "\n",
        "  return Data(charges=charges, x=positions, y=y, cell=cell, n=n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG8nOP-zpD7X"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NKYRvbLrQx8"
      },
      "source": [
        "We used Dragoni's dataset, which can be downloaded [here](https://archive.materialscloud.org/record/2017.0006/v2) in XYZ format (DB_bccFe_Dragoni.tar.gz)\n",
        "\n",
        "Place DB\\*.xyz files on the *path2data* folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TVsIsgvpJ_e"
      },
      "source": [
        "## Load only DB 1 (6001 structures expanded to 54 atoms each)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agZkxTfWpShu"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "DBs_to_load = [1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCUsRCZ6pUp0"
      },
      "source": [
        "## Load all 8 DBs (with DB1 expanded to 54 atoms each, others untouched)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG9qC4R4pHOm"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "DBs_to_load = [1, 2, 3, 4, 5, 6, 7, 8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Gl2SWgpfqa"
      },
      "source": [
        "## Pre-Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vthGS5zlph73"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "from ase.io import read\n",
        "\n",
        "data_list = []\n",
        "\n",
        "dbnames = [\"DB{}.xyz\".format(n) for n in DBs_to_load]\n",
        "fns = [ path2data + n for n in dbnames ]\n",
        "for fn in fns:\n",
        "  db = read(fn, index=\":\")\n",
        "  data_list += list(map(data_object, db))\n",
        "\n",
        "print(len(data_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plHW4EIJpsX7"
      },
      "source": [
        "# Generate mean / std and create train / test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-6-1HoGpycp"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "Y_atom = torch.tensor([r.y / r.n for r in data_list], dtype=DTYPE)\n",
        "\n",
        "y_atom_mean = Y_atom.mean().item()\n",
        "y_atom_std = Y_atom.std().item()\n",
        "\n",
        "print(y_atom_mean, y_atom_std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSY8LtcPpz3a"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "random.shuffle(data_list)\n",
        "\n",
        "train_amount = int(len(data_list) * TRAINING_RATIO)\n",
        "train_data = data_list[:train_amount]\n",
        "test_data = data_list[train_amount:]\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds3hZ1RSp2xq"
      },
      "source": [
        "# DimeNet - Edited version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3vZcl5qA1F"
      },
      "source": [
        "**Changes done:**\n",
        "\n",
        "*   Added support to Periodic Boundary Conditions\n",
        "*   Added support to energy de-standardization\n",
        "*   Fixed a mistake made on the PyTorch Geometric implementation on the Embedding Block\n",
        "*   Fixed a mistake made on the original implementation of angles (swapped indices)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgDzCCmmp8lI"
      },
      "source": [
        "#@title  { vertical-output: true, form-width: \"25%\" }\n",
        "from torch_geometric.nn import DimeNet\n",
        "from torch_geometric.nn.acts import swish\n",
        "from math import sqrt, pi as PI\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import Linear, Embedding\n",
        "from torch_scatter import scatter\n",
        "from torch_sparse import SparseTensor\n",
        "from torch_geometric.nn import radius_graph\n",
        "from torch_geometric.data import download_url\n",
        "from torch_geometric.data.makedirs import makedirs\n",
        "\n",
        "from torch_geometric.nn.models.dimenet import Envelope\n",
        "from torch_geometric.nn.models.dimenet import BesselBasisLayer\n",
        "from torch_geometric.nn.models.dimenet import SphericalBasisLayer\n",
        "from torch_geometric.nn.models.dimenet import ResidualLayer\n",
        "from torch_geometric.nn.models.dimenet import InteractionBlock\n",
        "from torch_geometric.nn.models.dimenet import OutputBlock\n",
        "\n",
        "from torch_geometric.nn.models.dimenet_utils import bessel_basis, real_sph_harm\n",
        "\n",
        "try:\n",
        "    import sympy as sym\n",
        "except ImportError:\n",
        "    sym = None\n",
        "\n",
        "import os\n",
        "try:\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "    import tensorflow as tf\n",
        "except ImportError:\n",
        "    tf = None\n",
        "\n",
        "\n",
        "qm9_target_dict = {\n",
        "    0: 'mu',\n",
        "    1: 'alpha',\n",
        "    2: 'homo',\n",
        "    3: 'lumo',\n",
        "    5: 'r2',\n",
        "    6: 'zpve',\n",
        "    7: 'U0',\n",
        "    8: 'U',\n",
        "    9: 'H',\n",
        "    10: 'G',\n",
        "    11: 'Cv',\n",
        "}\n",
        "\n",
        "# Implement PBC\n",
        "from ase.neighborlist import neighbor_list \n",
        "from ase import Atoms\n",
        "\n",
        "\n",
        "class EmbeddingBlock(torch.nn.Module):\n",
        "    def __init__(self, num_radial, hidden_channels, act=swish):\n",
        "        super(EmbeddingBlock, self).__init__()\n",
        "        self.act = act\n",
        "\n",
        "        self.emb = Embedding(95, hidden_channels)\n",
        "        self.lin_rbf = Linear(num_radial, hidden_channels)\n",
        "        self.lin = Linear(3 * hidden_channels, hidden_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.emb.weight.data.uniform_(-sqrt(3), sqrt(3))\n",
        "        self.lin_rbf.reset_parameters()\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x, rbf, i, j):\n",
        "        x = self.emb(x)\n",
        "        #rbf = self.act(self.lin_rbf(rbf)) # FIX: this should not have an activation function\n",
        "        rbf = self.lin_rbf(rbf)\n",
        "        return self.act(self.lin(torch.cat([x[i], x[j], rbf], dim=-1)))\n",
        "\n",
        "class DimeNet2(DimeNet):\n",
        "  \n",
        "  def __init__(self, hidden_channels, out_channels, num_blocks, num_bilinear,\n",
        "                 num_spherical, num_radial, cutoff=5.0, envelope_exponent=5,\n",
        "                 num_before_skip=1, num_after_skip=2, num_output_layers=3,\n",
        "                 act=swish,\n",
        "                 mean=None, std=None):\n",
        "          super(DimeNet, self).__init__()\n",
        "\n",
        "          self.cutoff = cutoff\n",
        "\n",
        "          #set mean and standard deviation of energies\n",
        "          self.mean = mean \n",
        "          self.std = std\n",
        "\n",
        "          # padding used for PBCs\n",
        "          self.padding = torch.nn.ConstantPad2d((0,6,0,0), 0)\n",
        "\n",
        "          if sym is None:\n",
        "              raise ImportError('Package `sympy` could not be found.')\n",
        "\n",
        "          self.num_blocks = num_blocks\n",
        "\n",
        "          self.rbf = BesselBasisLayer(num_radial, cutoff, envelope_exponent)\n",
        "          self.sbf = SphericalBasisLayer(num_spherical, num_radial, cutoff,\n",
        "                                          envelope_exponent)\n",
        "\n",
        "          self.emb = EmbeddingBlock(num_radial, hidden_channels, act)\n",
        "\n",
        "          self.output_blocks = torch.nn.ModuleList([\n",
        "              OutputBlock(num_radial, hidden_channels, out_channels,\n",
        "                          num_output_layers, act) for _ in range(num_blocks + 1)\n",
        "          ])\n",
        "\n",
        "          self.interaction_blocks = torch.nn.ModuleList([\n",
        "              InteractionBlock(hidden_channels, num_bilinear, num_spherical,\n",
        "                                num_radial, num_before_skip, num_after_skip, act)\n",
        "              for _ in range(num_blocks)\n",
        "          ])\n",
        "\n",
        "          self.reset_parameters()\n",
        "\n",
        "  def pbc_edges(self, z, pos, cell, batch):\n",
        "          if cell is None:\n",
        "            return\n",
        "\n",
        "          tmp_z = z.cpu()\n",
        "          tmp_pos = pos.cpu()\n",
        "          tmp_cell = cell.cpu()\n",
        "          nh1_tmp = np.array([]) # will contain all connection from node i\n",
        "          nh2_tmp = np.array([]) # .. to node j\n",
        "          dist_tmp = np.array([]) # distances between (i,j)\n",
        "          shift_cells_tmp = None # bravais lattice multiplied by shift for the connection\n",
        "\n",
        "          if batch is not None: #batch input\n",
        "            tmp_batch = np.array(batch.cpu())\n",
        "            batch_size = []\n",
        "            found_b = []\n",
        "            for b in tmp_batch: # create an array with each element being the dim of the corresponding index batch\n",
        "              if b not in found_b:\n",
        "                found_b.append(b)\n",
        "                batch_size.append((tmp_batch == b).sum())\n",
        "\n",
        "            for i in range(len(batch_size)):\n",
        "              prev_sum = sum(batch_size[:i])\n",
        "              current_z = tmp_z[prev_sum:batch_size[i]+prev_sum]\n",
        "              # create the atomic structure\n",
        "              atms = Atoms(charges=current_z, \n",
        "                           positions=tmp_pos[prev_sum:batch_size[i]+prev_sum], \n",
        "                           cell=tmp_cell[3*i:3*(i+1)], pbc=True) \n",
        "\n",
        "              # get the connections for the atomic structure w/ distances and shift\n",
        "              nh1, nh2, dist, shift = neighbor_list(\"ijdS\", atms, \n",
        "                                             self.cutoff, \n",
        "                                             self_interaction=False) \n",
        "\n",
        "              nh1 = nh1 + prev_sum # adds the number of previous elements to the atom index\n",
        "              nh2 = nh2 + prev_sum\n",
        "\n",
        "              nh1_tmp = np.concatenate((nh1_tmp, np.array(nh1)))\n",
        "              nh2_tmp = np.concatenate((nh2_tmp, np.array(nh2)))\n",
        "              dist_tmp = np.concatenate((dist_tmp, np.array(dist)))\n",
        "\n",
        "              # Mult cells array (9 elements each) for each connection element in the batch\n",
        "              cell_arr = np.asarray(tmp_cell[3*i:3*(i+1)]).reshape(-1)\n",
        "              repeat = np.tile(cell_arr, (len(dist), 1))\n",
        "\n",
        "              # multiply cell values by shift\n",
        "              repeat[:, 0:3] = (repeat[:, 0:3].T * shift[:, 0]).T\n",
        "              repeat[:, 3:6] = (repeat[:, 3:6].T * shift[:, 1]).T\n",
        "              repeat[:, 6:9] = (repeat[:, 6:9].T * shift[:, 2]).T\n",
        "\n",
        "              if shift_cells_tmp  is None:\n",
        "                shift_cells_tmp  = np.matrix(repeat)\n",
        "              else:\n",
        "                shift_cells_tmp  = np.concatenate((shift_cells_tmp, repeat))\n",
        "          else: # single cell input\n",
        "              # create the atomic structure\n",
        "              atms = Atoms(charges=tmp_z, \n",
        "                           positions=tmp_pos, \n",
        "                           cell=tmp_cell, pbc=True)\n",
        "\n",
        "              # get the connections for the atomic structure w/ distances and shift\n",
        "              nh1, nh2, dist, shift = neighbor_list(\"ijdS\", atms, \n",
        "                                             self.cutoff, \n",
        "                                             self_interaction=False)\n",
        "\n",
        "              nh1_tmp = np.concatenate((nh1_tmp, np.array(nh1)))\n",
        "              nh2_tmp = np.concatenate((nh2_tmp, np.array(nh2)))\n",
        "              dist_tmp = np.concatenate((dist_tmp, np.array(dist)))\n",
        "\n",
        "              # Mult cells array (9 elements each) for each connection element in the batch\n",
        "              cell_arr = np.asarray(tmp_cell).reshape(-1)\n",
        "              repeat = np.tile(cell_arr, (len(dist), 1))\n",
        "\n",
        "              # multiply cell values by shift\n",
        "              repeat[:,0:3] = (repeat[:, 0:3].T * shift[:, 0]).T\n",
        "              repeat[:,3:6] = (repeat[:, 3:6].T * shift[:, 1]).T\n",
        "              repeat[:,6:9] = (repeat[:, 6:9].T * shift[:, 2]).T\n",
        "\n",
        "              shift_cells_tmp = np.matrix(repeat)\n",
        "          return [torch.tensor(nh1_tmp, dtype = torch.long).to(z.device), \n",
        "                  torch.tensor(nh2_tmp, dtype = torch.long).to(z.device), \n",
        "                  torch.tensor(dist_tmp, dtype = DTYPE).to(z.device),\n",
        "                  torch.tensor(shift_cells_tmp, dtype = DTYPE).to(z.device)]\n",
        "            \n",
        "\n",
        "  def triplets(self, edge_index, num_nodes, shift_cells=None):\n",
        "        row, col = edge_index  # j->i\n",
        "\n",
        "        value = torch.arange(row.size(0), device=row.device)\n",
        "        adj_t = SparseTensor(row=col, col=row, value=value,\n",
        "                             sparse_sizes=(num_nodes, num_nodes))\n",
        "        adj_t_row = adj_t[row]\n",
        "\n",
        "        num_triplets = adj_t_row.set_value(None).sum(dim=1).to(torch.long)\n",
        "\n",
        "        # Node indices (k->j->i) for triplets.\n",
        "        idx_i = col.repeat_interleave(num_triplets)\n",
        "        idx_j = row.repeat_interleave(num_triplets)\n",
        "\n",
        "        if shift_cells is not None: # Update also the shift vectors\n",
        "          shift_cells = shift_cells.repeat_interleave(num_triplets, dim=0)\n",
        "\n",
        "        idx_k = adj_t_row.storage.col()\n",
        "\n",
        "        mask = (idx_i != idx_k)  # Remove i == k triplets.\n",
        "        idx_i, idx_j, idx_k = idx_i[mask], idx_j[mask], idx_k[mask]\n",
        "        if shift_cells is not None: # Remove also from the shift vector\n",
        "          shift_cells = shift_cells[mask]\n",
        "\n",
        "        # Edge indices (k-j, j->i) for triplets.\n",
        "        idx_kj = adj_t_row.storage.value()[mask]\n",
        "        idx_ji = adj_t_row.storage.row()[mask]\n",
        "\n",
        "        return col, row, idx_i, idx_j, idx_k, idx_kj, idx_ji, shift_cells\n",
        "\n",
        "  def forward(self, z, pos, cell=None, batch=None):\n",
        "        \n",
        "        edge_index = []\n",
        "        dist = []\n",
        "        shift_cells = None\n",
        "        if cell is not None: # implement PBC\n",
        "          r1, r2, dist, shift_cells = self.pbc_edges(z, pos, cell, batch)\n",
        "          edge_index = [r1, r2]\n",
        "        else: # old method without PBC\n",
        "          edge_index = radius_graph(pos, r=self.cutoff, batch=batch)\n",
        "\n",
        "        i, j, idx_i, idx_j, idx_k, idx_kj, idx_ji, shift_cells = self.triplets(\n",
        "            edge_index, num_nodes=z.size(0), shift_cells=shift_cells)        \n",
        "\n",
        "        # Calculate distances.\n",
        "        if cell is None: # calculate distance without PBC\n",
        "          dist = (pos[i] - pos[j]).pow(2).sum(dim=-1).sqrt()\n",
        "          \n",
        "        # Define atoms position \n",
        "        pos_i = pos[idx_i]\n",
        "        pos_j = pos[idx_j] # central atom\n",
        "        pos_k = pos[idx_k]\n",
        "        \n",
        "        if cell is not None: # Fix coordinates for PBCs\n",
        "          pos_i = pos_i + shift_cells[:, 0:3] + shift_cells[:, 3:6] + shift_cells[:, 6:9]\n",
        "          pos_k = pos_k + shift_cells[:, 0:3] + shift_cells[:, 3:6] + shift_cells[:, 6:9]\n",
        "\n",
        "        # Calculate angles - with some Fixes to indexes compared to the orig. version\n",
        "        pos_ji, pos_kj = pos_j - pos_i, pos_k - pos_j\n",
        "\n",
        "        a = (pos_ji * pos_kj).sum(dim=-1)\n",
        "        b = torch.cross(pos_ji, pos_kj).norm(dim=-1)\n",
        "        angle = torch.atan2(b, a)\n",
        "\n",
        "        rbf = self.rbf(dist)\n",
        "        sbf = self.sbf(dist, angle, idx_kj)\n",
        "\n",
        "        # Embedding block.\n",
        "        x = self.emb(z, rbf, i, j)\n",
        "        P = self.output_blocks[0](x, rbf, i, num_nodes=pos.size(0))\n",
        "\n",
        "        # Interaction blocks.\n",
        "        for interaction_block, output_block in zip(self.interaction_blocks,\n",
        "                                                   self.output_blocks[1:]):\n",
        "            x = interaction_block(x, rbf, sbf, idx_kj, idx_ji)\n",
        "            P += output_block(x, rbf, i, num_nodes=pos.size(0))\n",
        "\n",
        "        # Energy de-standardization\n",
        "        if self.std is not None and self.mean is not None:\n",
        "          P = P * self.std + self.mean\n",
        "\n",
        "        res = P.sum(dim=0) if batch is None else scatter(P, batch, dim=0)\n",
        "        return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQvcjWezqZof"
      },
      "source": [
        "# Training / Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvEPZD8Aqb_m"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "def experiment_summary():\n",
        "  print(\"DBs used:\", dbnames)\n",
        "  print(\"dtype:\", DTYPE)\n",
        "  print(\"mean:\", y_atom_mean, \", std:\", y_atom_std)\n",
        "  print(\"training ratio:\", TRAINING_RATIO)\n",
        "  print(\"batch size:\", BATCH_SIZE)\n",
        "  print(\"model:\", model)\n",
        "  print(\"optimizer:\", optimizer)\n",
        "  print(\"scheduler:\", scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qub6vw-qeg1"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "def train(model, loader, optimizer):\n",
        "  model.train()\n",
        "  rl = 0\n",
        "\n",
        "  for data in loader:\n",
        "    if WITH_PBC:\n",
        "      out = model(data.charges, data.x, data.cell, data.batch)\n",
        "    else:\n",
        "      out = model(data.charges, data.x, batch=data.batch)\n",
        "      \n",
        "    out = out.squeeze(1) \n",
        "    loss = CRITERION(out, data.y)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      rl += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "  return rl  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HH1gE7FqgdD"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "def test(model, loader):\n",
        "  model.eval()\n",
        "\n",
        "  maes = []\n",
        "  for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "    with torch.no_grad():\n",
        "      if WITH_PBC:\n",
        "        out = model(data.charges, data.x, data.cell, data.batch)\n",
        "      else:\n",
        "        out = model(data.charges, data.x, batch=data.batch)\n",
        "\n",
        "      out = out.squeeze(1)\n",
        "      mae = (out.view(-1) - data.y).abs()\n",
        "      maes.append(mae)\n",
        "  \n",
        "  mae = torch.cat(maes, dim=0) # flatten\n",
        "  return mae.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpdD5Z3YqiTg"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def save_mae_plot(tr_mae, te_mae, ep):\n",
        "  ep = range(1, len(tr_mae)+1)\n",
        "  plt.plot(ep, tr_mae, label=\"train mae\")\n",
        "  plt.plot(ep, te_mae, label=\"test mae\")\n",
        "  plt.ylabel('MAE')\n",
        "  plt.xlabel('EPOCHS')\n",
        "  plt.savefig(res_graphs_dir + 'mae_{}_.png'.format(ep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz9tRBdaql23"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "from numpy import savetxt\n",
        "\n",
        "def train_and_test(model, train_loader, test_loader, optimizer, scheduler, epochs=100):\n",
        "  experiment_summary()\n",
        "  train_maes = []\n",
        "  test_maes = []\n",
        "  for epoch in range(0, epochs):\n",
        "    loss = train(model, train_loader, optimizer)\n",
        "    train_acc = test(model, train_loader)\n",
        "    test_acc = test(model, test_loader)\n",
        "\n",
        "    if scheduler is not None:\n",
        "      scheduler.step(metrics=test_acc)\n",
        "\n",
        "    train_maes.append(train_acc)\n",
        "    test_maes.append(test_acc)\n",
        "\n",
        "    savetxt(res_maes_dir + \"train.txt\", train_maes, delimiter=\";\")\n",
        "    savetxt(res_maes_dir + \"test.txt\", test_maes, delimiter=\";\")\n",
        "    if epoch % 5 == 0: # save mae graph\n",
        "      save_mae_plot(train_maes, test_maes, epoch)\n",
        "      torch.save(model.state_dict(), res_models_dir + \"dimenet_{}.model\".format(epoch))\n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Train MAE: {train_acc:.4f}, Test MAE: {test_acc:.4f}, Train Loss: {loss:.4f}')\n",
        "    \n",
        "  return train_maes, test_maes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSXWWjl0qqIf"
      },
      "source": [
        "# Network Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PtQKPPwqs8-"
      },
      "source": [
        "#@title  { vertical-output: true }\n",
        "from datetime import datetime\n",
        "\n",
        "# hyperparameters \n",
        "EPOCHS = 100\n",
        "WITH_PBC = True\n",
        "hidden_channels = 128\n",
        "out_channels = 1\n",
        "num_blocks = 7\n",
        "num_bilinear = 8\n",
        "num_spherical = 7\n",
        "num_radial = 6\n",
        "cutoff = 3.5\n",
        "\n",
        "model = DimeNet2(hidden_channels=hidden_channels, out_channels=out_channels, num_blocks=num_blocks, num_bilinear=num_bilinear, num_spherical=num_spherical, num_radial=num_radial, cutoff=cutoff, std=y_atom_std, mean=y_atom_mean).to(device)\n",
        "\n",
        "# save the model data as string to be easily reused when loading the model\n",
        "model_str=\"model = DimeNet2(hidden_channels={}, out_channels={}, num_blocks={}, num_bilinear={}, num_spherical={}, num_radial={}, cutoff={}, std={}, mean={}).to(device)\".format(\n",
        "    hidden_channels, out_channels, num_blocks, num_bilinear, num_spherical, num_radial, cutoff, y_atom_std, y_atom_mean\n",
        ");\n",
        "model_file = open(resdir + \"model_str.txt\", \"w\")\n",
        "model_file.write(model_str)\n",
        "model_file.close()\n",
        "\n",
        "model = model.double() # necessary when using float64 as DTYPE\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = OPTIMIZER(model.parameters(), lr=0.0001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "    factor=0.1, patience=10, threshold=0.01, threshold_mode='abs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X65F-eUKq6-6"
      },
      "source": [
        "train_maes, test_maes = train_and_test(model, train_loader, test_loader, optimizer, scheduler, epochs = EPOCHS)\n",
        "\n",
        "torch.save(model.state_dict(), res_models_dir + \"final.model\") # save final model\n",
        "save_mae_plot(train_maes, test_maes, 9999) # save final mae plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APtIHa1UrGEx"
      },
      "source": [
        "## Function to clean the network from memory if you need to instanciate it again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPyeH_HJrLau"
      },
      "source": [
        "import gc\n",
        "\n",
        "if model:\n",
        "  model.cpu()\n",
        "  del model\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  print(\"Deleted dimenet\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}