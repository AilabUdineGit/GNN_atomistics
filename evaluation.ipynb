{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "evaluation_latest.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "k3YWso9Tq8A5",
        "VkKkQ4SazNBo",
        "7WPFM0kz2hoF",
        "a9k_s_cGsOFx",
        "u72xQ7qb3Pzm",
        "hKkdv8H13ROO",
        "DYCrdFtoMToP",
        "Ftak69mktBmL",
        "_CdrTElg0Nv4",
        "hQFduB3m0LW2",
        "mj9nfu2z2iXv",
        "UkP3MgpSLqk9",
        "bguTX5uomH-i",
        "Lv6RwcK-wFAk"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn5wj7eiq_vN"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onTIY9ydkJZK",
        "outputId": "25590df4-2574-4103-c5ef-4f0c3391e322"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"both\" }\n",
        "!pip install -q ase\n",
        "!pip install -q torch==1.8.0\n",
        "!pip install  torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu102.html\n",
        "!pip install -q torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cu102.html\n",
        "!pip install -q torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu102.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git@1.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.2 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 15 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hLooking in links: https://pytorch-geometric.com/whl/torch-1.8.0+cu102.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.8.0%2Bcu102/torch_scatter-2.0.8-cp37-cp37m-linux_x86_64.whl (8.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.8\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 704 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 376 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.0 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jek7aHUTkLSe"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import ase\n",
        "from pprint import pprint\n",
        "from torch_geometric.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_UUoU2HkMeQ"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # use gpu if available\n",
        "DTYPE = torch.float64\n",
        "ff = torch.tensor(54, dtype=DTYPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9qs3aiRsPg-"
      },
      "source": [
        "**Comment the first two lines of the following cell if you are not running this \n",
        "notebook on Colab**\n",
        "\n",
        "**Set BASE_PATH to the path to the folder where the files in the GNN_atomistics repository are located**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NC0JU_ckOps"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "BASE_PATH = '/content/drive/MyDrive/gnn_atomistics'\n",
        "DATA_PATH = BASE_PATH + \"/data/evaluation\"\n",
        "MODELS_PATH = BASE_PATH + \"/models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3YWso9Tq8A5"
      },
      "source": [
        "# Model definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1w1EZEV5PJY"
      },
      "source": [
        "from ase import Atoms\n",
        "from ase.neighborlist import neighbor_list \n",
        "\n",
        "def pbc_edges(cutoff, z, x, cell, batch, compute_sc=False):\n",
        "\n",
        "  NH1 = torch.tensor([], dtype=torch.long, device=DEVICE)\n",
        "  NH2 = torch.tensor([], dtype=torch.long, device=DEVICE)\n",
        "  S = torch.tensor([], dtype=torch.long, device=DEVICE)\n",
        "  D = torch.tensor([], dtype=DTYPE, device=DEVICE)\n",
        "  SC = torch.tensor([], dtype=DTYPE, device=DEVICE) if compute_sc else None\n",
        "  x_ = torch.clone(x).detach().cpu().numpy()\n",
        "\n",
        "  if batch is not None:\n",
        "    # count number of elements per batch\n",
        "    batch_ids = list(set(batch.cpu().tolist()))\n",
        "    batch_sizes = [ (batch == id).sum().item() for id in batch_ids ]\n",
        "\n",
        "    for i in range(len(batch_sizes)):\n",
        "      offset = sum(batch_sizes[:i]) # to obtain correct atom indices\n",
        "      \n",
        "      atoms = Atoms(charges = (z[offset:offset + batch_sizes[i]]).cpu(), \n",
        "        positions = x_[offset:offset + batch_sizes[i]], \n",
        "        cell = (cell[3*i:3*(i+1)]).cpu(),\n",
        "        pbc = True\n",
        "      ) \n",
        "      \n",
        "      nh1, nh2, s = neighbor_list(\"ijS\", atoms, cutoff, self_interaction=False) \n",
        "      nh1 = torch.tensor(nh1, dtype=torch.long, device=DEVICE)\n",
        "      nh2 = torch.tensor(nh2, dtype=torch.long, device=DEVICE)\n",
        "      nh1 = nh1 + offset\n",
        "      nh2 = nh2 + offset\n",
        "      s = torch.tensor(s, dtype=DTYPE, device=DEVICE)\n",
        "      d = x[nh2] - x[nh1] + torch.matmul(s, cell[3*i:3*(i+1)])\n",
        "      \n",
        "      if compute_sc:\n",
        "        cell_flat = torch.flatten(cell[3*i:3*(i+1)])\n",
        "        sc = torch.tile(cell_flat, (len(d), 1))\n",
        "        sc[:, 0:3] = (sc[:, 0:3].T * s[:, 0]).T\n",
        "        sc[:, 3:6] = (sc[:, 3:6].T * s[:, 1]).T\n",
        "        sc[:, 6:9] = (sc[:, 6:9].T * s[:, 2]).T\n",
        "        SC = torch.cat((SC, sc), 0)      \n",
        "\n",
        "      NH1 = torch.cat((NH1, nh1), 0)\n",
        "      NH2 = torch.cat((NH2, nh2), 0)\n",
        "      S = torch.cat((S, s), 0)\n",
        "      D = torch.cat((D, d), 0)\n",
        "\n",
        "  else: # no batch\n",
        "    atoms = Atoms(charges = z.cpu(), positions = x.cpu(), cell = cell.cpu(), pbc = True)\n",
        "    nh1, nh2, s = neighbor_list(\"ijS\", atoms, cutoff, self_interaction=False)\n",
        "    nh1 = torch.tensor(nh1, dtype=torch.long, device=DEVICE)\n",
        "    nh2 = torch.tensor(nh2, dtype=torch.long, device=DEVICE)\n",
        "    s = torch.tensor(s, dtype=DTYPE, device=DEVICE)\n",
        "    d = x[nh2] - x[nh1] + torch.matmul(s, cell)\n",
        "    \n",
        "    if compute_sc:\n",
        "      cell_flat = torch.flatten(cell)\n",
        "      sc = torch.tile(cell_flat, (len(d), 1))\n",
        "      sc[:, 0:3] = (sc[:, 0:3].T * s[:, 0]).T\n",
        "      sc[:, 3:6] = (sc[:, 3:6].T * s[:, 1]).T\n",
        "      sc[:, 6:9] = (sc[:, 6:9].T * s[:, 2]).T  \n",
        "      SC = sc\n",
        "\n",
        "    NH1, NH2, S, D = nh1, nh2, s, d\n",
        "\n",
        "  D = D.norm(dim=-1)\n",
        "  return  NH1, NH2, D, S, SC "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkKkQ4SazNBo"
      },
      "source": [
        "## Schnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV6g0S5g8JOR"
      },
      "source": [
        "import os\n",
        "import warnings\n",
        "import os.path as ospa\n",
        "from math import pi as PI\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Embedding, Sequential, Linear, ModuleList\n",
        "from torch_scatter import scatter\n",
        "from torch_geometric.data.makedirs import makedirs\n",
        "from torch_geometric.data import download_url, extract_zip\n",
        "from torch_geometric.nn import radius_graph, MessagePassing\n",
        "\n",
        "class SchNet2(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_channels=128, num_filters=128,\n",
        "                 num_interactions=6, num_gaussians=50, cutoff=10.0,\n",
        "                 readout='add', dipole=False, mean=None, std=None,\n",
        "                 atomref=None):\n",
        "        super(SchNetx, self).__init__()\n",
        "\n",
        "        assert readout in ['add', 'sum', 'mean']\n",
        "\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.num_filters = num_filters\n",
        "        self.num_interactions = num_interactions\n",
        "        self.num_gaussians = num_gaussians\n",
        "        self.cutoff = cutoff\n",
        "        self.readout = readout\n",
        "        self.dipole = dipole\n",
        "        self.readout = 'add' if self.dipole else self.readout\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.scale = None\n",
        "\n",
        "        atomic_mass = torch.from_numpy(ase.data.atomic_masses)\n",
        "        self.register_buffer('atomic_mass', atomic_mass)\n",
        "\n",
        "        self.embedding = Embedding(100, hidden_channels)\n",
        "        self.distance_expansion = GaussianSmearing(0.0, cutoff, num_gaussians)\n",
        "\n",
        "        self.interactions = ModuleList()\n",
        "        for _ in range(num_interactions):\n",
        "            block = InteractionBlock(hidden_channels, num_gaussians,\n",
        "                                     num_filters, cutoff)\n",
        "            self.interactions.append(block)\n",
        "\n",
        "        self.lin1 = Linear(hidden_channels, hidden_channels // 2)\n",
        "        self.act = ShiftedSoftplus()\n",
        "        self.lin2 = Linear(hidden_channels // 2, 1)\n",
        "\n",
        "        self.register_buffer('initial_atomref', atomref)\n",
        "        self.atomref = None\n",
        "        if atomref is not None:\n",
        "            self.atomref = Embedding(100, 1)\n",
        "            self.atomref.weight.data.copy_(atomref)\n",
        "        \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.embedding.reset_parameters()\n",
        "        for interaction in self.interactions:\n",
        "            interaction.reset_parameters()\n",
        "        torch.nn.init.xavier_uniform_(self.lin1.weight)\n",
        "        self.lin1.bias.data.fill_(0)\n",
        "        torch.nn.init.xavier_uniform_(self.lin2.weight)\n",
        "        self.lin2.bias.data.fill_(0)\n",
        "        if self.atomref is not None:\n",
        "            self.atomref.weight.data.copy_(self.initial_atomref)\n",
        "\n",
        "    def forward(self, z, x, cell=None, batch=None):\n",
        "        assert z.dim() == 1 and z.dtype == torch.long\n",
        "        batch = torch.zeros_like(z) if batch is None else batch\n",
        "\n",
        "        h = self.embedding(z)\n",
        "\n",
        "        if cell != None:\n",
        "          row, col, edge_weight, shift, _ = pbc_edges(self.cutoff, z, x, cell, batch, compute_sc=False)\n",
        "          edge_index = torch.stack((row, col))\n",
        "        else:\n",
        "          edge_index = radius_graph(x, r=self.cutoff, batch=batch)\n",
        "          row, col = edge_index\n",
        "          edge_weight = (x[row] - x[col]).norm(dim=-1)\n",
        "        \n",
        "        edge_attr = self.distance_expansion(edge_weight)\n",
        "\n",
        "        for interaction in self.interactions:\n",
        "            h = h + interaction(h, edge_index, edge_weight, edge_attr)\n",
        "\n",
        "        h = self.lin1(h)\n",
        "        h = self.act(h)\n",
        "        h = self.lin2(h)\n",
        "\n",
        "        if self.dipole:\n",
        "            mass = self.atomic_mass[z].view(-1, 1)\n",
        "            c = scatter(mass * x, batch, dim=0) / scatter(mass, batch, dim=0)\n",
        "            h = h * (pos - c[batch])\n",
        "\n",
        "        if not self.dipole and self.mean is not None and self.std is not None:\n",
        "            h = h * self.std + self.mean\n",
        "\n",
        "        if not self.dipole and self.atomref is not None:\n",
        "            h = h + self.atomref(z)\n",
        "\n",
        "        out = scatter(h, batch, dim=0, reduce=self.readout)\n",
        "        \n",
        "        if self.dipole:\n",
        "            out = torch.norm(out, dim=-1, keepdim=True)\n",
        "\n",
        "        if self.scale is not None:\n",
        "            out = self.scale * out\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f'{self.__class__.__name__}('\n",
        "                f'hidden_channels={self.hidden_channels}, '\n",
        "                f'num_filters={self.num_filters}, '\n",
        "                f'num_interactions={self.num_interactions}, '\n",
        "                f'num_gaussians={self.num_gaussians}, '\n",
        "                f'cutoff={self.cutoff})')\n",
        "\n",
        "\n",
        "\n",
        "class InteractionBlock(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, num_gaussians, num_filters, cutoff):\n",
        "        super(InteractionBlock, self).__init__()\n",
        "        self.mlp = Sequential(\n",
        "            Linear(num_gaussians, num_filters),\n",
        "            ShiftedSoftplus(),\n",
        "            Linear(num_filters, num_filters),\n",
        "        )\n",
        "        self.conv = CFConv(hidden_channels, hidden_channels, num_filters,\n",
        "                           self.mlp, cutoff)\n",
        "        self.act = ShiftedSoftplus()\n",
        "        self.lin = Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.mlp[0].weight)\n",
        "        self.mlp[0].bias.data.fill_(0)\n",
        "        torch.nn.init.xavier_uniform_(self.mlp[2].weight)\n",
        "        self.mlp[0].bias.data.fill_(0)\n",
        "        self.conv.reset_parameters()\n",
        "        torch.nn.init.xavier_uniform_(self.lin.weight)\n",
        "        self.lin.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, edge_attr):\n",
        "        x = self.conv(x, edge_index, edge_weight, edge_attr)\n",
        "        x = self.act(x)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CFConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, num_filters, nn, cutoff):\n",
        "        super(CFConv, self).__init__(aggr='add')\n",
        "        self.lin1 = Linear(in_channels, num_filters, bias=False)\n",
        "        self.lin2 = Linear(num_filters, out_channels)\n",
        "        self.nn = nn\n",
        "        self.cutoff = cutoff\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.lin1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.lin2.weight)\n",
        "        self.lin2.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, edge_attr):\n",
        "        C = 0.5 * (torch.cos(edge_weight * PI / self.cutoff) + 1.0)\n",
        "        W = self.nn(edge_attr) * C.view(-1, 1)\n",
        "\n",
        "        x = self.lin1(x)\n",
        "        x = self.propagate(edge_index, x=x, W=W)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "    def message(self, x_j, W):\n",
        "        return x_j * W\n",
        "\n",
        "\n",
        "class GaussianSmearing(torch.nn.Module):\n",
        "    def __init__(self, start=0.0, stop=5.0, num_gaussians=50):\n",
        "        super(GaussianSmearing, self).__init__()\n",
        "        offset = torch.linspace(start, stop, num_gaussians)\n",
        "        self.coeff = (-0.5 / (offset[1] - offset[0]).item()**2)\n",
        "        self.register_buffer('offset', offset)\n",
        "\n",
        "    def forward(self, dist):\n",
        "        dist = dist.view(-1, 1) - self.offset.view(1, -1)\n",
        "        res = torch.exp(self.coeff * torch.pow(dist, 2))\n",
        "        return res\n",
        "\n",
        "class ShiftedSoftplus(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShiftedSoftplus, self).__init__()\n",
        "        self.shift = torch.log(torch.tensor(2.0)).item()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.softplus(x) - self.shift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WPFM0kz2hoF"
      },
      "source": [
        "## Dimenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqkjj8bV2zHS"
      },
      "source": [
        "#@title  { vertical-output: true, form-width: \"25%\" }\n",
        "from torch_geometric.nn import DimeNet\n",
        "from torch_geometric.nn.acts import swish\n",
        "from math import sqrt, pi as PI\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import Linear, Embedding\n",
        "from torch_scatter import scatter\n",
        "from torch_sparse import SparseTensor\n",
        "from torch_geometric.nn import radius_graph\n",
        "from torch_geometric.data import download_url\n",
        "from torch_geometric.data.makedirs import makedirs\n",
        "\n",
        "from torch_geometric.nn.models.dimenet import Envelope\n",
        "from torch_geometric.nn.models.dimenet import BesselBasisLayer\n",
        "from torch_geometric.nn.models.dimenet import SphericalBasisLayer\n",
        "from torch_geometric.nn.models.dimenet import ResidualLayer\n",
        "from torch_geometric.nn.models.dimenet import InteractionBlock\n",
        "from torch_geometric.nn.models.dimenet import OutputBlock\n",
        "\n",
        "from torch_geometric.nn.models.dimenet_utils import bessel_basis, real_sph_harm\n",
        "import ase\n",
        "from ase.neighborlist import neighbor_list \n",
        "from ase import Atoms\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "try:\n",
        "    import sympy as sym\n",
        "except ImportError:\n",
        "    sym = None\n",
        "\n",
        "import os\n",
        "try:\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "    import tensorflow as tf\n",
        "except ImportError:\n",
        "    tf = None\n",
        "\n",
        "\n",
        "class EmbeddingBlock(torch.nn.Module):\n",
        "  def __init__(self, num_radial, hidden_channels, act=swish):\n",
        "    super(EmbeddingBlock, self).__init__()\n",
        "    self.act = act\n",
        "\n",
        "    self.emb = Embedding(95, hidden_channels)\n",
        "    self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
        "    self.lin = Linear(3 * hidden_channels, hidden_channels)\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    self.emb.weight.data.uniform_(-sqrt(3), sqrt(3))\n",
        "    self.lin_rbf.reset_parameters()\n",
        "    self.lin.reset_parameters()\n",
        "\n",
        "  def forward(self, x, rbf, i, j):\n",
        "    x = self.emb(x)\n",
        "    #rbf = self.act(self.lin_rbf(rbf)) # FIX: this should not have an activation function\n",
        "    rbf = self.lin_rbf(rbf)\n",
        "    return self.act(self.lin(torch.cat([x[i], x[j], rbf], dim=-1)))\n",
        "\n",
        "\n",
        "\n",
        "class DimeNet2(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, hidden_channels, out_channels, num_blocks, num_bilinear,\n",
        "                 num_spherical, num_radial, cutoff=5.0, envelope_exponent=5,\n",
        "                 num_before_skip=1, num_after_skip=2, num_output_layers=3,\n",
        "                 act=swish, mean=None, std=None):\n",
        "    super(DimeNetx, self).__init__()\n",
        "\n",
        "    self.cutoff = cutoff\n",
        "\n",
        "    #set mean and standard deviation of energies\n",
        "    self.mean = mean \n",
        "    self.std = std\n",
        "\n",
        "    # padding used for PBCs\n",
        "    self.padding = torch.nn.ConstantPad2d((0,6,0,0), 0)\n",
        "\n",
        "    if sym is None:\n",
        "        raise ImportError('Package `sympy` could not be found.')\n",
        "\n",
        "    self.num_blocks = num_blocks\n",
        "\n",
        "    self.rbf = BesselBasisLayer(num_radial, cutoff, envelope_exponent)\n",
        "    self.sbf = SphericalBasisLayer(num_spherical, num_radial, cutoff,\n",
        "                                    envelope_exponent)\n",
        "\n",
        "    self.emb = EmbeddingBlock(num_radial, hidden_channels, act)\n",
        "\n",
        "    self.output_blocks = torch.nn.ModuleList([\n",
        "        OutputBlock(num_radial, hidden_channels, out_channels,\n",
        "                    num_output_layers, act) for _ in range(num_blocks + 1)\n",
        "    ])\n",
        "\n",
        "    self.interaction_blocks = torch.nn.ModuleList([\n",
        "        InteractionBlock(hidden_channels, num_bilinear, num_spherical,\n",
        "                          num_radial, num_before_skip, num_after_skip, act)\n",
        "        for _ in range(num_blocks)\n",
        "    ])\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    self.rbf.reset_parameters()\n",
        "    self.emb.reset_parameters()\n",
        "    for out in self.output_blocks:\n",
        "      out.reset_parameters()\n",
        "    for interaction in self.interaction_blocks:\n",
        "      interaction.reset_parameters()\n",
        "\n",
        "  def triplets_original(self, edge_index, num_nodes):\n",
        "    row, col = edge_index  # j->i\n",
        "\n",
        "    value = torch.arange(row.size(0), device=row.device)\n",
        "    adj_t = SparseTensor(row=col, col=row, value=value,\n",
        "                         sparse_sizes=(num_nodes, num_nodes))\n",
        "    adj_t_row = adj_t[row]\n",
        "    num_triplets = adj_t_row.set_value(None).sum(dim=1).to(torch.long)\n",
        "\n",
        "    # Node indices (k->j->i) for triplets.\n",
        "    idx_i = col.repeat_interleave(num_triplets)\n",
        "    idx_j = row.repeat_interleave(num_triplets)\n",
        "    idx_k = adj_t_row.storage.col()\n",
        "    mask = (idx_i != idx_k)  # Remove i == k triplets.\n",
        "    idx_i, idx_j, idx_k = idx_i[mask], idx_j[mask], idx_k[mask]\n",
        "\n",
        "    # Edge indices (k-j, j->i) for triplets.\n",
        "    idx_kj = adj_t_row.storage.value()[mask]\n",
        "    idx_ji = adj_t_row.storage.row()[mask]\n",
        "\n",
        "    return col, row, idx_i, idx_j, idx_k, idx_kj, idx_ji\n",
        "\n",
        "\n",
        "  def triplets(self, edge_index, num_nodes, shift_cells=None, shift=None):\n",
        "    row, col = edge_index  # j->i\n",
        "\n",
        "    value = torch.arange(row.size(0), device=row.device)\n",
        "    adj_t = SparseTensor(row=col, col=row, value=value,\n",
        "                         sparse_sizes=(num_nodes, num_nodes))\n",
        "    adj_t_row = adj_t[row]\n",
        "    num_triplets = adj_t_row.set_value(None).sum(dim=1).to(torch.long)\n",
        "\n",
        "    idx_i = col.repeat_interleave(num_triplets)\n",
        "    idx_j = row.repeat_interleave(num_triplets)\n",
        "    idx_k = adj_t_row.storage.col()\n",
        "\n",
        "    if shift_cells is not None: # Update also the shift vectors\n",
        "      shift_cells_i = shift_cells.repeat_interleave(num_triplets, dim=0)\n",
        "      shift_i = shift.repeat_interleave(num_triplets, dim=0)\n",
        "      shift_cells_k = -shift_cells[adj_t_row.storage.value()]\n",
        "      shift_k = -shift[adj_t_row.storage.value()]\n",
        "\n",
        "    mask = torch.all((torch.cat((torch.unsqueeze(idx_i, 1), shift_i), dim=1) ==\\\n",
        "                      torch.cat((torch.unsqueeze(idx_k, 1), shift_k), dim=1)), dim=1)\n",
        "\n",
        "    idx_i, idx_j, idx_k = idx_i[~mask], idx_j[~mask], idx_k[~mask]\n",
        "    if shift_cells is not None: # Remove also from the shift vector\n",
        "      shift_cells_i = shift_cells_i[~mask]\n",
        "      shift_cells_k = shift_cells_k[~mask]\n",
        "      shift_i = shift_i[~mask]\n",
        "      shift_k = shift_k[~mask]\n",
        "\n",
        "    idx_kj = adj_t_row.storage.value()[~mask]\n",
        "    idx_ji = adj_t_row.storage.row()[~mask]\n",
        "\n",
        "    return col, row, idx_i, idx_j, idx_k, idx_kj, idx_ji, shift_cells_i, shift_i, shift_cells_k, shift_k\n",
        "\n",
        "\n",
        "  def forward(self, z, pos, cell=None, batch=None):\n",
        "\n",
        "    edge_index = []\n",
        "    dist = []\n",
        "    shift_cells = None      \n",
        "    \n",
        "    edge_index = radius_graph(pos, r=self.cutoff, batch=batch)\n",
        "    i, j, idx_i, idx_j, idx_k, idx_kj, idx_ji = self.triplets_original(\n",
        "      edge_index, num_nodes=z.size(0))        \n",
        "    dist = (pos[i] - pos[j]).pow(2).sum(dim=-1).sqrt()\n",
        "\n",
        "    # Define atoms position \n",
        "    pos_i = pos[idx_i]\n",
        "    pos_j = pos[idx_j]\n",
        "    pos_k = pos[idx_k]\n",
        "\n",
        "    # Calculate angles - with some Fixes to indexes compared to the orig. version\n",
        "    pos_ji, pos_kj = pos_j - pos_i, pos_k - pos_j\n",
        "    a = (pos_ji * pos_kj).sum(dim=-1)\n",
        "    b = torch.cross(pos_ji, pos_kj)\n",
        "    b = torch.linalg.norm(b + 1e-16, dim=-1)\n",
        "    angle = torch.atan2(b, a)      \n",
        "\n",
        "    rbf = self.rbf(dist)\n",
        "    sbf = self.sbf(dist, angle, idx_kj)\n",
        "\n",
        "    # Embedding block.\n",
        "    x = self.emb(z, rbf, i, j)\n",
        "    P = self.output_blocks[0](x, rbf, i, num_nodes=pos.size(0))\n",
        "\n",
        "    # Interaction blocks.\n",
        "    for interaction_block, output_block in zip(self.interaction_blocks,\n",
        "                                               self.output_blocks[1:]):\n",
        "      x = interaction_block(x, rbf, sbf, idx_kj, idx_ji)\n",
        "      a = output_block(x, rbf, i, num_nodes=pos.size(0))\n",
        "      P += a\n",
        "\n",
        "    # Energy de-standardization\n",
        "    if self.std is not None and self.mean is not None:\n",
        "      P = P * self.std + self.mean\n",
        "    \n",
        "    \n",
        "    return P, (P.sum(dim=0) if batch is None else scatter(P, batch, dim=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9k_s_cGsOFx"
      },
      "source": [
        "# Other function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOTrBhPbLZeU"
      },
      "source": [
        "def volume(coords):\n",
        "  a = np.array(coords)\n",
        "  a = a.reshape((3,3))\n",
        "  return np.linalg.det(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F80EEyu7u1l7"
      },
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "DTYPE = torch.float64\n",
        "FE_CHARGE = 26\n",
        "def data_object(atoms: ase.Atoms):\n",
        "  \n",
        "  n = atoms.get_global_number_of_atoms()\n",
        "\n",
        "  vol = volume(atoms.cell)\n",
        "\n",
        "  if n == 1 and DB1x54 == True:\n",
        "    atoms = extend_atoms(atoms, 54)\n",
        "\n",
        "  n = atoms.get_global_number_of_atoms()\n",
        "\n",
        "  cell = torch.tensor(atoms.cell, dtype=DTYPE).to(device)\n",
        "  positions = torch.tensor(atoms.get_positions(), dtype=DTYPE).to(device)\n",
        "  \n",
        "  charges = [ FE_CHARGE ] * len(positions)\n",
        "  charges = torch.tensor(charges, dtype=torch.long).to(device)\n",
        "\n",
        "  y = torch.tensor(atoms.info[\"energy\"], dtype=DTYPE).to(device)\n",
        "\n",
        "  return Data(charges=charges, x=positions, y=y, cell=cell, n=n, vol=vol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYVczuDku5j0"
      },
      "source": [
        "from ase import Atoms\n",
        "def data_object_da_vett(cell: list, positions: list, energy = 0.0):\n",
        "  atoms = Atoms(charges = [FE_CHARGE] * len(positions), positions=positions, cell=cell, info={ \"energy\": energy }, pbc=True)\n",
        "  return data_object(atoms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwQThOezs_dG"
      },
      "source": [
        "# Instantiate models and load pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u72xQ7qb3Pzm"
      },
      "source": [
        "## SchNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5qvy49Pp9R2"
      },
      "source": [
        "schnet_fn = \"schnet_PBC_best (15giu)\"\n",
        "schnet_name = schnet_fn\n",
        "model_data = torch.load(f\"{MODELS_PATH}/{schnet_fn}\", map_location=DEVICE)\n",
        "print(\"{} \\n {} \\nmean = {}, std = {}\".format(model_data[\"desc\"], model_data[\"str\"], model_data[\"mean\"], model_data[\"std\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJrEiwQjp__v"
      },
      "source": [
        "schnet = SchNet2(hidden_channels = 128, num_filters=128, num_gaussians=128, cutoff=5.0, \n",
        "                num_interactions=3, readout=\"sum\", mean = -3460.825847482401, std = 0.16576383029449515).double().to(DEVICE)\n",
        "schnet.load_state_dict(model_data[\"state\"])\n",
        "schnet.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKkdv8H13ROO"
      },
      "source": [
        "## DimeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMPEj7Vt3S38"
      },
      "source": [
        "dimenet_fn = \"dimenet_PBC_best (10giu_57)\"\n",
        "dimenet_name = dimenet_fn\n",
        "model_data = torch.load(f\"{MODELS_PATH}/{dimenet_fn}\", map_location=DEVICE) \n",
        "print(\"{} \\n {} \\nmean = {}, std = {}\".format(model_data[\"desc\"], model_data[\"str\"], model_data[\"mean\"], model_data[\"std\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOcg8WcVQUNx"
      },
      "source": [
        "dimenet = DimeNetx(hidden_channels=128, out_channels=1, num_blocks=7, num_bilinear=8, num_spherical=7, num_radial=6, cutoff=3.5, mean = -3460.8258474824015, std = 0.16576383029449682).double().to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWyMrf403uic"
      },
      "source": [
        "state = model_data[\"state\"]\n",
        "if list(state.keys())[0].find(\"module\") != -1: # change dict keys if model was trained with DataParallel\n",
        "  new_state = {}\n",
        "  for k, v in state.items():\n",
        "    new_k = k[k.find(\".\")+1:]\n",
        "    new_state[new_k] = v\n",
        "else:\n",
        "  new_state = state\n",
        "dimenet.load_state_dict(new_state)\n",
        "dimenet.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ziCaZpU05K2"
      },
      "source": [
        "# **Equation of state**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYCrdFtoMToP"
      },
      "source": [
        "## Define functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MAYQzUbMTC4"
      },
      "source": [
        "def eos_data():\n",
        "  a0 = 2.843877166\n",
        "  data_list = []\n",
        "  a_arr = np.append(np.arange(a0*0.98, a0*1.02, 0.0005*a0), a0)\n",
        "  a_arr = np.sort(a_arr)\n",
        "  for a in a_arr:\n",
        "    data_list.append(data_object_da_vett([[a, 0, 0], [0, a, 0], [a/2, a/2, a/2]], \n",
        "                                  [[0.0, 0.0, 0.0]]\n",
        "    ))\n",
        "  print(f\"generated {len(data_list)} points\")\n",
        "  return data_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzhUeLgHtX7W"
      },
      "source": [
        "def eos(model):\n",
        "  out_x = []\n",
        "  out_y = []\n",
        "  model.eval()\n",
        "  data_list = eos_data()\n",
        "  test_loader = DataLoader(data_list, batch_size=1, shuffle=False)\n",
        "  for data in test_loader:  # Iterate in batches over the training/test dataset.\n",
        "      e = model(data.z, data.x, cell=data.cell, batch=data.batch)\n",
        "      out_y.append(e.squeeze(1).item())\n",
        "      out_x.append(data.vol.item())\n",
        "      \n",
        "  return out_x, out_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfuFF5dquNN-"
      },
      "source": [
        "def equation_of_state(model_name, x_model, y_model, x_dft, y_dft):\n",
        "  plt.rcParams.update({'font.size':25})\n",
        "  plt.rcParams.update({'axes.titlesize':30})\n",
        "  plt.rcParams.update({'figure.titlesize':30})\n",
        "  plt.rcParams.update({'xtick.labelsize':15})\n",
        "  plt.rcParams.update({'ytick.labelsize':15})\n",
        "  plt.rcParams.update({'legend.fontsize':25})\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(16,10))\n",
        "  plt.cla()\n",
        "  ax.set_xlim(10.99, 12)\n",
        "  ax.set_ylim(-3460934.301071031, -3460914.9599393304)\n",
        "  ax.set_title(model_name)\n",
        "  plt.xlabel(\"V\")\n",
        "  plt.ylabel(\"E\")\n",
        "\n",
        "  ax.scatter(x_model, [(en*1000) for en in y_model], color=\"blue\", label=model_name)\n",
        "  ax.plot(x_model, [(en*1000) for en in y_model], color=\"blue\")\n",
        "\n",
        "  ax.scatter(x_dft, y_dft, color=\"black\", label=\"DFT\")\n",
        "  ax.plot(x_dft, y_dft, color=\"black\")\n",
        "\n",
        "  ax.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftak69mktBmL"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbJkJxwUQ6FL"
      },
      "source": [
        "eos_x_dft = torch.load(f\"{DATA_PATH}/equation-of-state/dft_v.pt\", map_location=DEVICE)\n",
        "eos_y_dft = torch.load(f\"{DATA_PATH}/equation-of-state/dft_e.pt\", map_location=DEVICE)\n",
        "eos_x_gap = torch.load(f\"{DATA_PATH}/equation-of-state/gap_v.pt\", map_location=DEVICE)\n",
        "eos_y_gap = torch.load(f\"{DATA_PATH}/equation-of-state/gap_e.pt\", map_location=DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEDax2Ld6TU3"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsI8ezW5wppf"
      },
      "source": [
        "### Schnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO0E7cLe6XR1"
      },
      "source": [
        "eos_x_schnet, eos_y_schnet = eos(schnet)\n",
        "equation_of_state(schnet_name, eos_x_schnet, eos_y_schnet, eos_x_dft, eos_y_dft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXFrpEfjwr1M"
      },
      "source": [
        "### Dimenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVIrVtlc4omO"
      },
      "source": [
        "eos_x_dimenet, eos_y_dimenet = eos(dimenet)\n",
        "equation_of_state(dimenet_name, eos_x_dimenet, eos_y_dimenet, eos_x_dft, eos_y_dft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jbSVkRE-Bqq"
      },
      "source": [
        "# **Bain path**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CdrTElg0Nv4"
      },
      "source": [
        "## Define functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKlkNYpa_a4i"
      },
      "source": [
        "import math\n",
        "from math import sqrt\n",
        "from torch_geometric.data import DataLoader\n",
        "def bain_vo(model, a_coeff_min=0.95, a_coeff_max=1.05):\n",
        "\n",
        "  a_0 = 2.83477\n",
        "\n",
        "  plot_x = []\n",
        "  plot_y = []\n",
        "  extra_x = []\n",
        "  extra_y = []\n",
        "\n",
        "  cas = []\n",
        "  for sr_ in np.arange(0.5, 2.05, 0.02):\n",
        "    sr = sr_.item()\n",
        "    data_list = []\n",
        "\n",
        "    r = sr**(2/3)\n",
        "    a_c = r**(-1)\n",
        "    a_tmp = np.arange(0.98*a_0,1.02*a_0,0.02)\n",
        "    a_tmp = a_tmp.tolist()\n",
        "    a_tmp.append(a_0)\n",
        "    tmp_arr = []\n",
        "    for current_a in a_tmp:\n",
        "      cell = [[math.sqrt(a_c)*current_a, 0, 0], [0, math.sqrt(a_c)*current_a, 0], [math.sqrt(a_c)*current_a/2, math.sqrt(a_c)*current_a/2, (r)*current_a/2]]\n",
        "      positions = [[0.0, 0.0, 0.0]]\n",
        "      data_list.append(data_object_da_vett(cell, positions, 0.0))\n",
        "\n",
        "    model.eval()\n",
        "    inputs = []\n",
        "    candidate_ys = []\n",
        "    test_loader = DataLoader(data_list, batch_size=1, shuffle=False)\n",
        "    for data in test_loader:  # Iterate in batches over the training/test dataset.\n",
        "      e = model(data.z, data.x, cell=data.cell, batch=batch)\n",
        "      candidate_ys.append(e.squeeze(1).item())\n",
        "      inputs.append(data)\n",
        "\n",
        "    plot_x.append(sr)\n",
        "    #plot_x.append(r/sqrt(1/r))\n",
        "    extra_x += [r/sqrt(1/r)]*len(candidate_ys)\n",
        "    #extra_x += [r]*len(candidate_ys)\n",
        "\n",
        "    #find the minimum energy and corresponding a value;\n",
        "    j = 0\n",
        "    ok = 0\n",
        "    for i in range(len(candidate_ys)):\n",
        "      if candidate_ys[i] < candidate_ys[j]:\n",
        "        j = i\n",
        "\n",
        "    cas.append(r)\n",
        "\n",
        "    plot_y.append(min(candidate_ys))\n",
        "\n",
        "    extra_y += candidate_ys\n",
        "\n",
        "  return plot_x, plot_y, extra_x, extra_y, cas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA-EaYy3Vt9r"
      },
      "source": [
        "def bain_cv(model):\n",
        "\n",
        "  a = 2.83477\n",
        "  #a = 2.8325\n",
        "\n",
        "  plot_x = []\n",
        "  plot_y = []\n",
        "  data_list = []\n",
        "  data = []\n",
        "\n",
        "  for sr_ in np.arange(0.5, 2, 0.02):\n",
        "    sr = sr_.item()\n",
        "    r = sr**(2/3)\n",
        "    a_c = r**(-1)\n",
        "\n",
        "    cell = [[sqrt(a_c)*a,0,0],[0,sqrt(a_c)*a,0],[sqrt(a_c)*a/2,sqrt(a_c)*a/2,(r)*a/2]]\n",
        "    positions = [[0.0,0.0,0.0]]\n",
        "    data_list.append(data_object_da_vett(cell, positions, 0.0))\n",
        "\n",
        "    #plot_x.append(r)\n",
        "    plot_x.append(r/sqrt(1/r))\n",
        "  \n",
        "  test_loader = DataLoader(data_list, batch_size=1, shuffle=False)\n",
        "  \n",
        "  for data in test_loader:  # Iterate in batches over the training/test dataset.\n",
        "      e = model(data.z, data.x, cell=data.cell, batch=data.batch)\n",
        "      plot_y.append(e.squeeze(1).item())\n",
        "\n",
        "  return plot_x, plot_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTZItLfaulB0"
      },
      "source": [
        "def bain_path(model_name, calc_mode, model_x, model_y, dft_x, dft_y):\n",
        "  plt.rcParams.update({'font.size':25})\n",
        "  plt.rcParams.update({'axes.titlesize':30})\n",
        "  plt.rcParams.update({'figure.titlesize':30})\n",
        "  plt.rcParams.update({'xtick.labelsize':15})\n",
        "  plt.rcParams.update({'ytick.labelsize':15})\n",
        "  plt.rcParams.update({'legend.fontsize':20})\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(16,10))\n",
        "  plt.cla()\n",
        "  ax.set_xlim(0.7, 2.05)\n",
        "  ax.set_ylim(min([y*1000 for y in model_y]) -50, -3.4605e6 + 90)\n",
        "  ax.set_title(model_name + \" \" + calc_mode)\n",
        "  plt.xlabel(\"c/a\")\n",
        "  plt.ylabel(\"E\")\n",
        "\n",
        "  plt.axvline(x=0.8, linestyle='--', label=\"bct\")\n",
        "  plt.axvline(x=1.0, linestyle='--', color='orange', label=\"bcc\")\n",
        "  plt.axvline(x=math.sqrt(2), linestyle='--', color='brown', label=\"fcc\")\n",
        "\n",
        "  ax.scatter(dft_x, [en for en in dft_y], color=\"black\", label=\"DFT\")\n",
        "  ax.plot(dft_x, [en for en in dft_y], color=\"black\")\n",
        "\n",
        "  ax.scatter(model_x, [(en*1000) for en in model_y], color=\"blue\")\n",
        "  ax.plot(model_x, [(en*1000) for en in model_y], color=\"blue\", label=model_name + \" \" + calc_mode)\n",
        "\n",
        "  ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQFduB3m0LW2"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCFjfB0KdNHO"
      },
      "source": [
        "a = open(f\"{DATA_PATH}/bain-path/DFT.csv\", \"r\").read()\n",
        "d = a.replace(\";\", \"\").replace(\",\",\".\").split()\n",
        "bain_x_dft = [float(x) for i, x in enumerate(d) if i%2 == 0]\n",
        "bain_y_dft = [float(x) for i, x in enumerate(d) if i%2 == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hphKHPY52frQ"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf5kKJ3Zwu1h"
      },
      "source": [
        "### Schnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZqPDR8t1Xgn"
      },
      "source": [
        "bain_vo_x_schnet, bain_vo_y_schnet, _, _, _ = bain_vo(schnet)\n",
        "bain_path(schnet_name, \"VO\", bain_vo_x_schnet, bain_vo_y_schnet, bain_x_dft, bain_y_dft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvl15wfI1ami"
      },
      "source": [
        "bain_cv_x_schnet, bain_cv_y_schnet = bain_cv(schnet)\n",
        "bain_path(schnet_name, \"CV\", bain_cv_x_schnet, bain_cv_y_schnet, bain_x_dft, bain_y_dft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEr3ynfiwwJr"
      },
      "source": [
        "### Dimenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjVftDFS1gLb"
      },
      "source": [
        "bain_vo_x_dimenet, bain_vo_y_dimenet, _, _, _ = bain_vo(dimenet)\n",
        "bain_path(dimenet_name, \"VO\", bain_vo_x_dimenet, bain_vo_y_dimenet, bain_x_dft, bain_y_dft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzE3Ntv61oPg"
      },
      "source": [
        "bain_cv_x_dimenet, bain_cv_y_dimenet = bain_cv(dimenet)\n",
        "bain_path(dimenet_name, \"CV\", bain_cv_x_dimenet, bain_cv_y_dimenet, bain_x_dft, bain_y_dft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd7ebzKoJ14B"
      },
      "source": [
        "# **Vacancy formation energy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj9nfu2z2iXv"
      },
      "source": [
        "## Define functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHOxk5lP2kpA"
      },
      "source": [
        "def eval_vacancy(model, loader):\n",
        "  ycap = []\n",
        "  for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "      e = model(data.z, data.x, cell=data.cell, batch=data.batch)\n",
        "      ycap.append(e.squeeze(1).item())  \n",
        "  return ycap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIBngIno5Oo6"
      },
      "source": [
        "def vacancy_formation_energy(model_name, N, E_total, E_total_v):\n",
        "  print(model_name)\n",
        "  print(\"{} -> {}\".format(N, N-1))\n",
        "  print(\"E_total_v =\", E_total_v)\n",
        "  print(\"E_total = \", E_total)\n",
        "  print(\"E_v =\", E_total_v - (E_total*(N-1)/N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkP3MgpSLqk9"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRVr5X9ALbD2"
      },
      "source": [
        "#bravais lattice vectors\n",
        "vac_128_lattice_str = \"\"\"\n",
        " 11.3360000   0.0000000   0.0000000 \n",
        "  0.0000000  11.3360000   0.0000000 \n",
        "  0.0000000   0.0000000  11.3360000 \n",
        "\"\"\" \n",
        "#sites\n",
        "vac_128_sites_str = \"\"\"\n",
        "Fe      0.0000000   0.0000000   0.0000000\n",
        "Fe      1.4170000   1.4170000   1.4170000\n",
        "Fe      8.5020000   0.0000000   2.8340000\n",
        "Fe      5.6680000   5.6680000   0.0000000\n",
        "Fe      8.5020000   2.8340000   0.0000000\n",
        "Fe      2.8340000   2.8340000   5.6680000\n",
        "Fe      2.8340000   8.5020000   8.5020000\n",
        "Fe      0.0000000   0.0000000   8.5020000\n",
        "Fe      5.6680000   5.6680000   8.5020000\n",
        "Fe      0.0000000   2.8340000   5.6680000\n",
        "Fe      5.6680000   0.0000000   5.6680000\n",
        "Fe      8.5020000   2.8340000   8.5020000\n",
        "Fe      0.0000000   5.6680000   2.8340000\n",
        "Fe      2.8340000   0.0000000   5.6680000\n",
        "Fe      8.5020000   5.6680000   5.6680000\n",
        "Fe      0.0000000   8.5020000   0.0000000\n",
        "Fe      2.8340000   2.8340000   2.8340000\n",
        "Fe      8.5020000   8.5020000   2.8340000\n",
        "Fe      2.8340000   5.6680000   0.0000000\n",
        "Fe      5.6680000   8.5020000   5.6680000\n",
        "Fe      5.6680000   0.0000000   2.8340000\n",
        "Fe      5.6680000   2.8340000   0.0000000\n",
        "Fe      5.6680000   2.8340000   2.8340000\n",
        "Fe      8.5020000   0.0000000   0.0000000\n",
        "Fe      0.0000000   8.5020000   8.5020000\n",
        "Fe      2.8340000   5.6680000   8.5020000\n",
        "Fe      2.8340000   8.5020000   5.6680000\n",
        "Fe      0.0000000   0.0000000   5.6680000\n",
        "Fe      5.6680000   2.8340000   8.5020000\n",
        "Fe      8.5020000   0.0000000   8.5020000\n",
        "Fe      0.0000000   2.8340000   2.8340000\n",
        "Fe      5.6680000   5.6680000   5.6680000\n",
        "Fe      8.5020000   2.8340000   5.6680000\n",
        "Fe      0.0000000   5.6680000   0.0000000\n",
        "Fe      2.8340000   0.0000000   2.8340000\n",
        "Fe      8.5020000   5.6680000   2.8340000\n",
        "Fe      2.8340000   2.8340000   0.0000000\n",
        "Fe      5.6680000   8.5020000   2.8340000\n",
        "Fe      8.5020000   8.5020000   0.0000000\n",
        "Fe      5.6680000   0.0000000   0.0000000\n",
        "Fe      0.0000000   5.6680000   8.5020000\n",
        "Fe      0.0000000   8.5020000   5.6680000\n",
        "Fe      2.8340000   2.8340000   8.5020000\n",
        "Fe      8.5020000   8.5020000   8.5020000\n",
        "Fe      2.8340000   5.6680000   5.6680000\n",
        "Fe      5.6680000   0.0000000   8.5020000\n",
        "Fe      2.8340000   8.5020000   2.8340000\n",
        "Fe      0.0000000   0.0000000   2.8340000\n",
        "Fe      5.6680000   2.8340000   5.6680000\n",
        "Fe      8.5020000   0.0000000   5.6680000\n",
        "Fe      0.0000000   2.8340000   0.0000000\n",
        "Fe      5.6680000   5.6680000   2.8340000\n",
        "Fe      8.5020000   2.8340000   2.8340000\n",
        "Fe      2.8340000   0.0000000   0.0000000\n",
        "Fe      5.6680000   8.5020000   0.0000000\n",
        "Fe      8.5020000   5.6680000   0.0000000\n",
        "Fe      0.0000000   2.8340000   8.5020000\n",
        "Fe      5.6680000   8.5020000   8.5020000\n",
        "Fe      0.0000000   5.6680000   5.6680000\n",
        "Fe      2.8340000   0.0000000   8.5020000\n",
        "Fe      8.5020000   5.6680000   8.5020000\n",
        "Fe      0.0000000   8.5020000   2.8340000\n",
        "Fe      8.5020000   8.5020000   5.6680000\n",
        "Fe      2.8340000   5.6680000   2.8340000\n",
        "Fe      2.8340000   8.5020000   0.0000000\n",
        "Fe      9.9190000   1.4170000   4.2510000\n",
        "Fe      7.0850000   7.0850000   1.4170000\n",
        "Fe      9.9190000   4.2510000   1.4170000\n",
        "Fe      4.2510000   4.2510000   7.0850000\n",
        "Fe      4.2510000   9.9190000   9.9190000\n",
        "Fe      1.4170000   1.4170000   9.9190000\n",
        "Fe      7.0850000   7.0850000   9.9190000\n",
        "Fe      1.4170000   4.2510000   7.0850000\n",
        "Fe      7.0850000   1.4170000   7.0850000\n",
        "Fe      9.9190000   4.2510000   9.9190000\n",
        "Fe      1.4170000   7.0850000   4.2510000\n",
        "Fe      4.2510000   1.4170000   7.0850000\n",
        "Fe      9.9190000   7.0850000   7.0850000\n",
        "Fe      1.4170000   9.9190000   1.4170000\n",
        "Fe      4.2510000   4.2510000   4.2510000\n",
        "Fe      9.9190000   9.9190000   4.2510000\n",
        "Fe      4.2510000   7.0850000   1.4170000\n",
        "Fe      7.0850000   9.9190000   7.0850000\n",
        "Fe      7.0850000   1.4170000   4.2510000\n",
        "Fe      7.0850000   4.2510000   1.4170000\n",
        "Fe      7.0850000   4.2510000   4.2510000\n",
        "Fe      9.9190000   1.4170000   1.4170000\n",
        "Fe      1.4170000   9.9190000   9.9190000\n",
        "Fe      4.2510000   7.0850000   9.9190000\n",
        "Fe      4.2510000   9.9190000   7.0850000\n",
        "Fe      1.4170000   1.4170000   7.0850000\n",
        "Fe      7.0850000   4.2510000   9.9190000\n",
        "Fe      9.9190000   1.4170000   9.9190000\n",
        "Fe      1.4170000   4.2510000   4.2510000\n",
        "Fe      7.0850000   7.0850000   7.0850000\n",
        "Fe      9.9190000   4.2510000   7.0850000\n",
        "Fe      1.4170000   7.0850000   1.4170000\n",
        "Fe      4.2510000   1.4170000   4.2510000\n",
        "Fe      9.9190000   7.0850000   4.2510000\n",
        "Fe      4.2510000   4.2510000   1.4170000\n",
        "Fe      7.0850000   9.9190000   4.2510000\n",
        "Fe      9.9190000   9.9190000   1.4170000\n",
        "Fe      7.0850000   1.4170000   1.4170000\n",
        "Fe      1.4170000   7.0850000   9.9190000\n",
        "Fe      1.4170000   9.9190000   7.0850000\n",
        "Fe      4.2510000   4.2510000   9.9190000\n",
        "Fe      9.9190000   9.9190000   9.9190000\n",
        "Fe      4.2510000   7.0850000   7.0850000\n",
        "Fe      7.0850000   1.4170000   9.9190000\n",
        "Fe      4.2510000   9.9190000   4.2510000\n",
        "Fe      1.4170000   1.4170000   4.2510000\n",
        "Fe      7.0850000   4.2510000   7.0850000\n",
        "Fe      9.9190000   1.4170000   7.0850000\n",
        "Fe      1.4170000   4.2510000   1.4170000\n",
        "Fe      7.0850000   7.0850000   4.2510000\n",
        "Fe      9.9190000   4.2510000   4.2510000\n",
        "Fe      4.2510000   1.4170000   1.4170000\n",
        "Fe      7.0850000   9.9190000   1.4170000\n",
        "Fe      9.9190000   7.0850000   1.4170000\n",
        "Fe      1.4170000   4.2510000   9.9190000\n",
        "Fe      7.0850000   9.9190000   9.9190000\n",
        "Fe      1.4170000   7.0850000   7.0850000\n",
        "Fe      4.2510000   1.4170000   9.9190000\n",
        "Fe      9.9190000   7.0850000   9.9190000\n",
        "Fe      1.4170000   9.9190000   4.2510000\n",
        "Fe      9.9190000   9.9190000   7.0850000\n",
        "Fe      4.2510000   7.0850000   4.2510000\n",
        "Fe      4.2510000   9.9190000   1.4170000\"\"\"\n",
        "\n",
        "vac_127_lattice_str = \"\"\"\n",
        " 11.3360000   0.0000000   0.0000000 \n",
        "  0.0000000  11.3360000   0.0000000 \n",
        "  0.0000000   0.0000000  11.3360000 \n",
        "\"\"\"\n",
        "\n",
        "vac_127_sites_str = \"\"\"\n",
        "Fe      1.4170000   1.4170000   1.4170000\n",
        "Fe      8.5020000   0.0000000   2.8340000\n",
        "Fe      5.6680000   5.6680000   0.0000000\n",
        "Fe      8.5020000   2.8340000   0.0000000\n",
        "Fe      2.8340000   2.8340000   5.6680000\n",
        "Fe      2.8340000   8.5020000   8.5020000\n",
        "Fe      0.0000000   0.0000000   8.5020000\n",
        "Fe      5.6680000   5.6680000   8.5020000\n",
        "Fe      0.0000000   2.8340000   5.6680000\n",
        "Fe      5.6680000   0.0000000   5.6680000\n",
        "Fe      8.5020000   2.8340000   8.5020000\n",
        "Fe      0.0000000   5.6680000   2.8340000\n",
        "Fe      2.8340000   0.0000000   5.6680000\n",
        "Fe      8.5020000   5.6680000   5.6680000\n",
        "Fe      0.0000000   8.5020000   0.0000000\n",
        "Fe      2.8340000   2.8340000   2.8340000\n",
        "Fe      8.5020000   8.5020000   2.8340000\n",
        "Fe      2.8340000   5.6680000   0.0000000\n",
        "Fe      5.6680000   8.5020000   5.6680000\n",
        "Fe      5.6680000   0.0000000   2.8340000\n",
        "Fe      5.6680000   2.8340000   0.0000000\n",
        "Fe      5.6680000   2.8340000   2.8340000\n",
        "Fe      8.5020000   0.0000000   0.0000000\n",
        "Fe      0.0000000   8.5020000   8.5020000\n",
        "Fe      2.8340000   5.6680000   8.5020000\n",
        "Fe      2.8340000   8.5020000   5.6680000\n",
        "Fe      0.0000000   0.0000000   5.6680000\n",
        "Fe      5.6680000   2.8340000   8.5020000\n",
        "Fe      8.5020000   0.0000000   8.5020000\n",
        "Fe      0.0000000   2.8340000   2.8340000\n",
        "Fe      5.6680000   5.6680000   5.6680000\n",
        "Fe      8.5020000   2.8340000   5.6680000\n",
        "Fe      0.0000000   5.6680000   0.0000000\n",
        "Fe      2.8340000   0.0000000   2.8340000\n",
        "Fe      8.5020000   5.6680000   2.8340000\n",
        "Fe      2.8340000   2.8340000   0.0000000\n",
        "Fe      5.6680000   8.5020000   2.8340000\n",
        "Fe      8.5020000   8.5020000   0.0000000\n",
        "Fe      5.6680000   0.0000000   0.0000000\n",
        "Fe      0.0000000   5.6680000   8.5020000\n",
        "Fe      0.0000000   8.5020000   5.6680000\n",
        "Fe      2.8340000   2.8340000   8.5020000\n",
        "Fe      8.5020000   8.5020000   8.5020000\n",
        "Fe      2.8340000   5.6680000   5.6680000\n",
        "Fe      5.6680000   0.0000000   8.5020000\n",
        "Fe      2.8340000   8.5020000   2.8340000\n",
        "Fe      0.0000000   0.0000000   2.8340000\n",
        "Fe      5.6680000   2.8340000   5.6680000\n",
        "Fe      8.5020000   0.0000000   5.6680000\n",
        "Fe      0.0000000   2.8340000   0.0000000\n",
        "Fe      5.6680000   5.6680000   2.8340000\n",
        "Fe      8.5020000   2.8340000   2.8340000\n",
        "Fe      2.8340000   0.0000000   0.0000000\n",
        "Fe      5.6680000   8.5020000   0.0000000\n",
        "Fe      8.5020000   5.6680000   0.0000000\n",
        "Fe      0.0000000   2.8340000   8.5020000\n",
        "Fe      5.6680000   8.5020000   8.5020000\n",
        "Fe      0.0000000   5.6680000   5.6680000\n",
        "Fe      2.8340000   0.0000000   8.5020000\n",
        "Fe      8.5020000   5.6680000   8.5020000\n",
        "Fe      0.0000000   8.5020000   2.8340000\n",
        "Fe      8.5020000   8.5020000   5.6680000\n",
        "Fe      2.8340000   5.6680000   2.8340000\n",
        "Fe      2.8340000   8.5020000   0.0000000\n",
        "Fe      9.9190000   1.4170000   4.2510000\n",
        "Fe      7.0850000   7.0850000   1.4170000\n",
        "Fe      9.9190000   4.2510000   1.4170000\n",
        "Fe      4.2510000   4.2510000   7.0850000\n",
        "Fe      4.2510000   9.9190000   9.9190000\n",
        "Fe      1.4170000   1.4170000   9.9190000\n",
        "Fe      7.0850000   7.0850000   9.9190000\n",
        "Fe      1.4170000   4.2510000   7.0850000\n",
        "Fe      7.0850000   1.4170000   7.0850000\n",
        "Fe      9.9190000   4.2510000   9.9190000\n",
        "Fe      1.4170000   7.0850000   4.2510000\n",
        "Fe      4.2510000   1.4170000   7.0850000\n",
        "Fe      9.9190000   7.0850000   7.0850000\n",
        "Fe      1.4170000   9.9190000   1.4170000\n",
        "Fe      4.2510000   4.2510000   4.2510000\n",
        "Fe      9.9190000   9.9190000   4.2510000\n",
        "Fe      4.2510000   7.0850000   1.4170000\n",
        "Fe      7.0850000   9.9190000   7.0850000\n",
        "Fe      7.0850000   1.4170000   4.2510000\n",
        "Fe      7.0850000   4.2510000   1.4170000\n",
        "Fe      7.0850000   4.2510000   4.2510000\n",
        "Fe      9.9190000   1.4170000   1.4170000\n",
        "Fe      1.4170000   9.9190000   9.9190000\n",
        "Fe      4.2510000   7.0850000   9.9190000\n",
        "Fe      4.2510000   9.9190000   7.0850000\n",
        "Fe      1.4170000   1.4170000   7.0850000\n",
        "Fe      7.0850000   4.2510000   9.9190000\n",
        "Fe      9.9190000   1.4170000   9.9190000\n",
        "Fe      1.4170000   4.2510000   4.2510000\n",
        "Fe      7.0850000   7.0850000   7.0850000\n",
        "Fe      9.9190000   4.2510000   7.0850000\n",
        "Fe      1.4170000   7.0850000   1.4170000\n",
        "Fe      4.2510000   1.4170000   4.2510000\n",
        "Fe      9.9190000   7.0850000   4.2510000\n",
        "Fe      4.2510000   4.2510000   1.4170000\n",
        "Fe      7.0850000   9.9190000   4.2510000\n",
        "Fe      9.9190000   9.9190000   1.4170000\n",
        "Fe      7.0850000   1.4170000   1.4170000\n",
        "Fe      1.4170000   7.0850000   9.9190000\n",
        "Fe      1.4170000   9.9190000   7.0850000\n",
        "Fe      4.2510000   4.2510000   9.9190000\n",
        "Fe      9.9190000   9.9190000   9.9190000\n",
        "Fe      4.2510000   7.0850000   7.0850000\n",
        "Fe      7.0850000   1.4170000   9.9190000\n",
        "Fe      4.2510000   9.9190000   4.2510000\n",
        "Fe      1.4170000   1.4170000   4.2510000\n",
        "Fe      7.0850000   4.2510000   7.0850000\n",
        "Fe      9.9190000   1.4170000   7.0850000\n",
        "Fe      1.4170000   4.2510000   1.4170000\n",
        "Fe      7.0850000   7.0850000   4.2510000\n",
        "Fe      9.9190000   4.2510000   4.2510000\n",
        "Fe      4.2510000   1.4170000   1.4170000\n",
        "Fe      7.0850000   9.9190000   1.4170000\n",
        "Fe      9.9190000   7.0850000   1.4170000\n",
        "Fe      1.4170000   4.2510000   9.9190000\n",
        "Fe      7.0850000   9.9190000   9.9190000\n",
        "Fe      1.4170000   7.0850000   7.0850000\n",
        "Fe      4.2510000   1.4170000   9.9190000\n",
        "Fe      9.9190000   7.0850000   9.9190000\n",
        "Fe      1.4170000   9.9190000   4.2510000\n",
        "Fe      9.9190000   9.9190000   7.0850000\n",
        "Fe      4.2510000   7.0850000   4.2510000\n",
        "Fe      4.2510000   9.9190000   1.4170000\"\"\"\n",
        "\n",
        "vac_53_lattice_str = \"\"\"\n",
        " 8.5020000   0.0000000   0.0000000\n",
        " 0.0000000   8.5020000   0.0000000\n",
        " 0.0000000   0.0000000   8.5020000\n",
        "\"\"\"\n",
        "\n",
        "vac_53_sites_str = \"\"\"\n",
        "Fe            1.3710331141        1.3710328145        1.3710330239\n",
        "Fe            2.8636873181        0.0000006056        0.0000002850\n",
        "Fe            5.6383130622        0.0000006007        0.0000002554\n",
        "Fe           -0.0000003236        0.0000005923        5.6383131764\n",
        "Fe           -0.0000003588        5.6780135759        2.8239863030\n",
        "Fe            2.8239871194        0.0000007435        5.6780131819\n",
        "Fe            5.6780131857        0.0000007299        5.6780132388\n",
        "Fe            2.8341486131        5.6678518988        2.8341480191\n",
        "Fe            5.6678516811        5.6678519082        2.8341480047\n",
        "Fe           -0.0000003414        2.8636857854        0.0000003193\n",
        "Fe            2.8239869847        2.8239859741        0.0000003364\n",
        "Fe            5.6780133287        2.8239860021        0.0000002954\n",
        "Fe           -0.0000003894        2.8239858259        5.6780134613\n",
        "Fe            2.8341485983        2.8341475140        5.6678517358\n",
        "Fe            5.6678517287        2.8341475206        5.6678518095\n",
        "Fe           -0.0000002984        0.0000005459        2.8636863926\n",
        "Fe           -0.0000003480        5.6383133214        0.0000003529\n",
        "Fe            2.8239870666        0.0000006725        2.8239865633\n",
        "Fe            5.6780131849        0.0000006603        2.8239865538\n",
        "Fe            2.8239870623        5.6780134537        0.0000003985\n",
        "Fe            5.6780132879        5.6780134462        0.0000003540\n",
        "Fe           -0.0000004008        5.6780135795        5.6780133363\n",
        "Fe            2.8341486930        5.6678518824        5.6678515993\n",
        "Fe            5.6678516714        5.6678518959        5.6678516707\n",
        "Fe           -0.0000003469        2.8239859294        2.8239862830\n",
        "Fe            2.8341485324        2.8341475875        2.8341479943\n",
        "Fe            5.6678517451        2.8341475991        2.8341479618\n",
        "Fe            4.2510003036        1.4201541600        1.4201542876\n",
        "Fe            7.1309666303        1.3710328093        1.3710329475\n",
        "Fe            1.3710331187        1.3710327648        7.1309672952\n",
        "Fe            1.4201542656        7.0818467019        4.2509996322\n",
        "Fe            4.2510003506        1.4201541416        7.0818459679\n",
        "Fe            7.1309665941        1.3710327507        7.1309673430\n",
        "Fe            4.2510003140        7.0939924882        4.2509997047\n",
        "Fe            7.0818453833        7.0818466683        4.2509996776\n",
        "Fe            1.4201540444        4.2509993812        1.4201540671\n",
        "Fe            4.2510003236        4.2509994657        1.4080084858\n",
        "Fe            7.0818455796        4.2509993973        1.4201540235\n",
        "Fe            1.4201540621        4.2509993032        7.0818463141\n",
        "Fe            4.2510003715        4.2509993925        7.0939919123\n",
        "Fe            7.0818455294        4.2509993185        7.0818463189\n",
        "Fe            1.4201542182        1.4201539663        4.2509997132\n",
        "Fe            1.3710331908        7.1309676815        1.3710331340\n",
        "Fe            4.2510002920        1.4080083865        4.2509997882\n",
        "Fe            7.0818454339        1.4201539908        4.2509997531\n",
        "Fe            4.2510003261        7.0818465133        1.4201543806\n",
        "Fe            7.1309665364        7.1309676832        1.3710330651\n",
        "Fe            1.3710332453        7.1309677188        7.1309671530\n",
        "Fe            4.2510003731        7.0818465930        7.0818459245\n",
        "Fe            7.1309664843        7.1309677507        7.1309672224\n",
        "Fe            1.4080083930        4.2509994420        4.2509996333\n",
        "Fe            4.2510003258        4.2509994018        4.2509996692\n",
        "Fe            7.0939911399        4.2509994633        4.2509996798\n",
        "\"\"\"\n",
        "\n",
        "vac_54_lattice_str = \"\"\"\n",
        " 8.5020000   0.0000000   0.0000000\n",
        " 0.0000000   8.5020000   0.0000000\n",
        " 0.0000000   0.0000000   8.5020000\n",
        "\"\"\"\n",
        "\n",
        "vac_54_sites_str = \"\"\"\n",
        "Fe      0.0000000   0.0000000   0.0000000\n",
        "Fe      1.4170000   1.4170000   1.4170000\n",
        "Fe      2.8340000   0.0000000   0.0000000\n",
        "Fe      5.6680000   0.0000000   0.0000000\n",
        "Fe      0.0000000   0.0000000   5.6680000\n",
        "Fe      0.0000000   5.6680000   2.8340000\n",
        "Fe      2.8340000   0.0000000   5.6680000\n",
        "Fe      5.6680000   0.0000000   5.6680000\n",
        "Fe      2.8340000   5.6680000   2.8340000\n",
        "Fe      5.6680000   5.6680000   2.8340000\n",
        "Fe      0.0000000   2.8340000   0.0000000\n",
        "Fe      2.8340000   2.8340000   0.0000000\n",
        "Fe      5.6680000   2.8340000   0.0000000\n",
        "Fe      0.0000000   2.8340000   5.6680000\n",
        "Fe      2.8340000   2.8340000   5.6680000\n",
        "Fe      5.6680000   2.8340000   5.6680000\n",
        "Fe      0.0000000   0.0000000   2.8340000\n",
        "Fe      0.0000000   5.6680000   0.0000000\n",
        "Fe      2.8340000   0.0000000   2.8340000\n",
        "Fe      5.6680000   0.0000000   2.8340000\n",
        "Fe      2.8340000   5.6680000   0.0000000\n",
        "Fe      5.6680000   5.6680000   0.0000000\n",
        "Fe      0.0000000   5.6680000   5.6680000\n",
        "Fe      2.8340000   5.6680000   5.6680000\n",
        "Fe      5.6680000   5.6680000   5.6680000\n",
        "Fe      0.0000000   2.8340000   2.8340000\n",
        "Fe      2.8340000   2.8340000   2.8340000\n",
        "Fe      5.6680000   2.8340000   2.8340000\n",
        "Fe      4.2510000   1.4170000   1.4170000\n",
        "Fe      7.0850000   1.4170000   1.4170000\n",
        "Fe      1.4170000   1.4170000   7.0850000\n",
        "Fe      1.4170000   7.0850000   4.2510000\n",
        "Fe      4.2510000   1.4170000   7.0850000\n",
        "Fe      7.0850000   1.4170000   7.0850000\n",
        "Fe      4.2510000   7.0850000   4.2510000\n",
        "Fe      7.0850000   7.0850000   4.2510000\n",
        "Fe      1.4170000   4.2510000   1.4170000\n",
        "Fe      4.2510000   4.2510000   1.4170000\n",
        "Fe      7.0850000   4.2510000   1.4170000\n",
        "Fe      1.4170000   4.2510000   7.0850000\n",
        "Fe      4.2510000   4.2510000   7.0850000\n",
        "Fe      7.0850000   4.2510000   7.0850000\n",
        "Fe      1.4170000   1.4170000   4.2510000\n",
        "Fe      1.4170000   7.0850000   1.4170000\n",
        "Fe      4.2510000   1.4170000   4.2510000\n",
        "Fe      7.0850000   1.4170000   4.2510000\n",
        "Fe      4.2510000   7.0850000   1.4170000\n",
        "Fe      7.0850000   7.0850000   1.4170000\n",
        "Fe      1.4170000   7.0850000   7.0850000\n",
        "Fe      4.2510000   7.0850000   7.0850000\n",
        "Fe      7.0850000   7.0850000   7.0850000\n",
        "Fe      1.4170000   4.2510000   4.2510000\n",
        "Fe      4.2510000   4.2510000   4.2510000\n",
        "Fe      7.0850000   4.2510000   4.2510000\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owfVuaNTuvfs"
      },
      "source": [
        "def from_lattice_sites(lattice_str, sites_str):\n",
        "  lattice = list(map(float, lattice_str.strip().split()))\n",
        "  sites = sites_str.strip().split(\"\\n\")\n",
        "  sites = [x.replace(\"Fe\", \"\").strip().split() for x in sites]\n",
        "  sites = [ list(map(float, x)) for x in sites ]\n",
        "  print(len(lattice), len(sites))\n",
        "  return lattice, sites"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcKWWKQPvU4c"
      },
      "source": [
        "vac_128_lattice, vac_128_sites = from_lattice_sites(vac_128_lattice_str, vac_128_sites_str) \n",
        "vac_127_lattice, vac_127_sites = from_lattice_sites(vac_127_lattice_str, vac_127_sites_str) \n",
        "reshape = lambda x: [[x[0], x[1], x[2]],[x[3], x[4],x[5]], [x[6], x[7], x[8]] ]\n",
        "vac_127_lattice = reshape(vac_127_lattice)\n",
        "vac_128_lattice = reshape(vac_128_lattice)\n",
        "vac_128 = data_object_da_vett(vac_128_lattice, vac_128_sites, 0.0)\n",
        "vac_127 = data_object_da_vett(vac_127_lattice, vac_127_sites, 0.0)\n",
        "\n",
        "vac_54_lattice, vac_54_sites = from_lattice_sites(vac_54_lattice_str, vac_54_sites_str) \n",
        "vac_53_lattice, vac_53_sites = from_lattice_sites(vac_53_lattice_str, vac_53_sites_str) \n",
        "vac_53_lattice = reshape(vac_53_lattice)\n",
        "vac_54_lattice = reshape(vac_54_lattice)\n",
        "vac_54 = data_object_da_vett(vac_54_lattice, vac_54_sites, 0.0)\n",
        "vac_53 = data_object_da_vett(vac_53_lattice, vac_53_sites, 0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9oJhouCvV5S"
      },
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "loader_128 = DataLoader([vac_128, vac_127], batch_size=1, shuffle=False)\n",
        "loader_54 = DataLoader([vac_54, vac_53], batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqtyLTEq1xFf"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGfgMd3qw0TE"
      },
      "source": [
        "### Schnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRpXG2r8vmEP"
      },
      "source": [
        "E_total, E_total_v = eval_vacancy(schnet, loader_54)\n",
        "vacancy_formation_energy(schnet_name, 54, E_total, E_total_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9blphPK5vpGJ"
      },
      "source": [
        "E_total, E_total_v = eval_vacancy(schnet, loader_128)\n",
        "vacancy_formation_energy(schnet_name, 128, E_total, E_total_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW1REXaJw1fF"
      },
      "source": [
        "### Dimenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvsO0Tknbjtx"
      },
      "source": [
        "E_total, E_total_v = eval_vacancy(dimenet, loader_54)\n",
        "vacancy_formation_energy(dimenet_name, 54, E_total, E_total_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb6BCOggObVH"
      },
      "source": [
        "E_total, E_total_v = eval_vacancy(dimenet, loader_128)\n",
        "vacancy_formation_energy(dimenet_name, 128, E_total, E_total_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqHBv7rWJpPm"
      },
      "source": [
        "# **Surface energy**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bguTX5uomH-i"
      },
      "source": [
        "## Define functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDudPb3KYvp-"
      },
      "source": [
        "import re\n",
        "def get_cell(configuration_string):\n",
        "  for line in configuration_string.split(\"\\n\"):\n",
        "    if line.find(\"xlo xhi\") != -1:\n",
        "      xlo, xhi = [float(v) for v in re.sub(\"\\s+\",\" \", line).strip().split(\" \")[:2]]\n",
        "    if line.find(\"ylo yhi\") != -1:\n",
        "      ylo, yhi = [float(v) for v in re.sub(\"\\s+\",\" \", line).strip().split(\" \")[:2]]\n",
        "    if line.find(\"zlo zhi\") != -1:\n",
        "      zlo, zhi = [float(v) for v in re.sub(\"\\s+\",\" \", line).strip().split(\" \")[:2]]\n",
        "  return [[xhi-xlo, 0.0, 0.0], [0.0, yhi-ylo, 0.0], [0.0, 0.0, zhi-zlo]]\n",
        "\n",
        "def get_positions(configuration_string):\n",
        "  configuration_string = configuration_string.split(\"Atoms\")[1]\n",
        "  positions = []\n",
        "  for line in configuration_string.split(\"\\n\")[2:-1]:\n",
        "    xyz = re.sub(\"\\s+\",\" \", line).strip().split(\" \")[2:]\n",
        "    positions.append([float(v) for v in xyz])\n",
        "  return positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Q7kucUdaUh"
      },
      "source": [
        "def data_object_from_lmpfile(fn):\n",
        "  with open(fn) as f:\n",
        "    s = f.read()\n",
        "  dobj = data_object_da_vett(get_cell(s), get_positions(s))\n",
        "  return dobj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZMS7evLv8cL"
      },
      "source": [
        "def eV_to_J(e):\n",
        "  return e / 6.242e+18"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "povJo_Lav9o_"
      },
      "source": [
        "def A_to_m(l):\n",
        "  return l / 1e+10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm0Gg_lPv_RK"
      },
      "source": [
        "def evaluate(model, data_list):\n",
        "  loader = DataLoader(data_list, batch_size=1, shuffle=False)\n",
        "  model.eval()\n",
        "  ycap = []\n",
        "  for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "      e = model(data.z, data.x, cell=data.cell, batch=data.batch)\n",
        "      ycap.append(e.squeeze(1).item())  \n",
        "  return ycap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUbJplFHwA6r"
      },
      "source": [
        "def surface_energy(model, model_name, bulk, surf, id):\n",
        "  print(model_name)\n",
        "  \n",
        "  A = bulk.cell[1][1].item() * bulk.cell[2][2].item()\n",
        "  print(f\"A_{id}: {A}\")\n",
        "  \n",
        "  E_bulk, E_surf = evaluate(model, [bulk, surf])\n",
        "  print(f\"E_bulk_{id}: {E_bulk}\")\n",
        "  print(f\"E_surf_{id}: {E_surf}\")\n",
        "\n",
        "  gamma_surf = (E_surf - E_bulk) / (2 * A)\n",
        "\n",
        "  E_bulk_J, E_surf_J = eV_to_J(E_bulk), eV_to_J(E_surf)\n",
        "  A_m = A_to_m(bulk.cell[1][1].item()) * A_to_m(bulk.cell[2][2].item())\n",
        "  gamma_surf_J_m2 = (E_surf_J - E_bulk_J) / (2 * A_m)\n",
        "\n",
        "  print(f\"gamma_surf_{id}: {gamma_surf} (eV/A^2) = {gamma_surf_J_m2} (J/m^2)\")\n",
        "  return gamma_surf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv6RwcK-wFAk"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha1wuqW6wHEs"
      },
      "source": [
        "SURF_PATH = f\"{DATA_PATH}/surface-energy/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNeby_J_wVwb"
      },
      "source": [
        "bulk_100 = data_object_from_lmpfile(SURF_PATH + \"lmpmodel_100\")\n",
        "surf_100 = data_object_from_lmpfile(SURF_PATH + \"lmpmodel_100_surf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3qS7Pbtw_VL"
      },
      "source": [
        "bulk_110 = data_object_from_lmpfile(SURF_PATH + \"lmpmodel_110\")\n",
        "surf_110 = data_object_from_lmpfile(SURF_PATH + \"lmpmodel_110_surf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWHhoYjgxDLx"
      },
      "source": [
        "bulk_111 = data_object_from_lmpfile(SURF_PATH + \"lmpmodel_111\")\n",
        "surf_111 = data_object_from_lmpfile(SURF_PATH + \"lmpmodel_111_surf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRCh2UoRxEFF"
      },
      "source": [
        "bulk_112 = data_object_from_lmpfile(SURF_PATH + \"lmpmodel_112\")\n",
        "surf_112 = data_object_from_lmpfile(SURF_PATH + \"lmpmodel_112_surf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB6dlv_LmQgJ"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_kXggdjxHkQ"
      },
      "source": [
        "### Schnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJs59Cd6xHNm"
      },
      "source": [
        "gamma_surf_100 = surface_energy(schnet, schnet_name, bulk_100, surf_100, \"100\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fEMef2exK8p"
      },
      "source": [
        "gamma_surf_110 = surface_energy(schnet, schnet_name, bulk_110, surf_110, \"110\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3KmA2acxLpf"
      },
      "source": [
        "gamma_surf_111 = surface_energy(schnet, schnet_name, bulk_111, surf_111, \"111\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpQMfDj_xMTY"
      },
      "source": [
        "gamma_surf_112 = surface_energy(schnet, schnet_name, bulk_112, surf_112, \"112\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH4yunhvxJHP"
      },
      "source": [
        "### Dimenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvnT2Poomhx8"
      },
      "source": [
        "gamma_surf_100 = surface_energy(dimenet, dimenet_name, bulk_100, surf_100, \"100\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqwHsJEOph8Z"
      },
      "source": [
        "gamma_surf_110 = surface_energy(dimenet, dimenet_name, bulk_110, surf_110, \"110\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt7EH-EKpl8x"
      },
      "source": [
        "gamma_surf_111 = surface_energy(dimenet, dimenet_name, bulk_111, surf_111, \"111\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeINx9KVpp-7"
      },
      "source": [
        "gamma_surf_112 = surface_energy(dimenet, dimenet_name, bulk_112, surf_112, \"112\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}