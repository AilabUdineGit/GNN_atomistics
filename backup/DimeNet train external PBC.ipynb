{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"DimeNet train external PBC","provenance":[{"file_id":"1Nnh907B78ol1gjWzQclnT3dMo4-LAi8s","timestamp":1620119975899}],"collapsed_sections":["IeZyCbkfkuXx"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LCADQ_Y-5AZ2"},"source":["*   Make sure you run this using a GPU (On Google Colab: Runtime -> Change Runtime Type -> GPU)"]},{"cell_type":"markdown","metadata":{"id":"IeZyCbkfkuXx"},"source":["# Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mNXgZoCHklOe","outputId":"4d1c9df2-ffc3-4a64-c25f-32d3d1e40b7b"},"source":["#@title  { vertical-output: true, display-mode: \"both\" }\n","!pip install -q ase\n","!pip install -q torch==1.8.0\n","!pip install  torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu102.html\n","!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu102.html\n","!pip install -q torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cu102.html\n","!pip install -q torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu102.html\n","!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git@1.7.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 2.2MB 8.8MB/s \n","\u001b[K     |████████████████████████████████| 735.5MB 24kB/s \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LI7ky8iYk18J"},"source":["import numpy as np\n","import torch\n","import ase\n","import random\n","from torch_geometric.data import Data, DataLoader\n","from ase.io import read"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LIT8KZO9rdrG"},"source":["SEED = 55555\n","\n","np.random.seed(SEED)\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-9NRhR_rfne"},"source":["## Set training hyperparameters"]},{"cell_type":"code","metadata":{"id":"6Zgbvaqek4Kb"},"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # use GPU if available\n","DTYPE = torch.float64  # data type to use for data and model\n","\n","BASE_PATH = '/content/drive/MyDrive/gnn_atomistics'  # path to git repo/colab/gnn_atomistics\n","\n","TRAINING_RATIO = 0.8  # percent of the dataset to use for training\n","OPTIMIZER = torch.optim.Adam  \n","BATCH_SIZE = 4\n","CUTOFF = 3.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pjbYYGWrn5Xf"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","DATA_PATH = BASE_PATH + \"/data/training\"\n","MODELS_PATH = BASE_PATH + \"/models\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D_aZDovaoq7J"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"M0TTCoXzowQi"},"source":["## Define functions"]},{"cell_type":"code","metadata":{"id":"dum8n-V9oyw_"},"source":["M = {}\n","def extend_atoms(atoms, source, target):\n","  global M\n","  if source not in M.keys():\n","    M[source] = {}\n","  if target not in M[source].keys():\n","    M[source][target] = ase.build.find_optimal_cell_shape(atoms.get_cell(), target, \"sc\") \n","  supercell = ase.build.make_supercell(atoms, M[source][target])\n","  supercell.info[\"energy\"] = atoms.info[\"energy\"] * int(target / source)\n","  return supercell"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJNeHV88o0TT"},"source":["def data_object(atoms: ase.Atoms):\n","\n","  n = atoms.get_global_number_of_atoms()\n","  if n == 1:\n","    atoms = extend_atoms(atoms, 1, 54)\n","    n = atoms.get_global_number_of_atoms()\n","\n","  cell = torch.tensor(atoms.cell, dtype=DTYPE, device=DEVICE)\n","  x = torch.tensor(atoms.get_positions(), dtype=DTYPE, device=DEVICE)\n","  z = torch.tensor(atoms.get_array(\"numbers\", copy=True), dtype=torch.long, device=DEVICE)\n","  y = torch.tensor(atoms.info[\"energy\"], dtype=DTYPE, device=DEVICE)\n","  f = torch.tensor(atoms.get_array(\"force\", copy=True), dtype=DTYPE, device=DEVICE)\n","\n","  return Data(z=z, x=x, cell=cell, y=y, f=f, n=n)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BTNFmzPVuEp3"},"source":["from ase.neighborlist import neighbor_list \n","from ase import Atoms\n","def data_object_with_ghosts(atoms: ase.Atoms, cutoff: float):\n","\n","  nlocal = atoms.get_global_number_of_atoms()\n","  if nlocal == 1:\n","    atoms = extend_atoms(atoms, 1, 54)\n","    nlocal = atoms.get_global_number_of_atoms()\n","\n","  i, j, S = neighbor_list(\"ijS\", atoms, cutoff, self_interaction=False)\n","  mask = ~(np.all(np.equal(S, np.array([0, 0, 0])), axis=1))\n","  x = atoms.get_positions()\n","  cell = atoms.get_cell()\n","\n","  k = []\n","  shifts = []\n","  for l, s in zip(j[mask].tolist(), S[mask].tolist()):\n","    u = [v for v, el in enumerate(k) if el == l]\n","    found = False\n","    for v in u:\n","      if shifts[v][0] == s[0] and shifts[v][1] == s[1] and shifts[v][2] == s[2]:\n","        found = True\n","        break\n","    if not found:\n","      k.append(l)\n","      shifts.append(s)\n","\n","  k = np.array(k)\n","  shifts = np.array(shifts)\n","\n","  ghost_x = x[k] + np.matmul(shifts, cell)\n","  new_x = np.concatenate((x, ghost_x))\n","  nghost = len(ghost_x)\n","  n = nlocal + nghost\n","\n","  idx_local = torch.tensor(list(range(nlocal)), dtype=torch.long, device=\"cpu\")\n","  cell = torch.tensor(cell, dtype=DTYPE, device=\"cpu\")\n","  x = torch.tensor(new_x, dtype=DTYPE, device=\"cpu\")\n","  z = torch.tensor([26]*(nlocal + nghost), dtype=torch.long, device=\"cpu\")\n","  y = torch.tensor(atoms.info[\"energy\"], dtype=DTYPE, device=\"cpu\")\n","  f = torch.tensor(atoms.get_array(\"force\", copy=True), dtype=DTYPE, device=\"cpu\")\n","\n","  return Data(z=z, x=x, cell=cell, y=y, f=f, idx_local=idx_local, nlocal=nlocal, nghost=nghost)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DG8nOP-zpD7X"},"source":["## Load data"]},{"cell_type":"code","metadata":{"id":"vthGS5zlph73"},"source":["data = []\n","file_names = [f\"DB{i}.xyz\" for i in range(1,9)]\n","file_paths = [f\"{DATA_PATH}/{f}\" for f in file_names]\n","for f in file_paths:\n","  db = read(f, index=\":\")\n","  data += list(map(lambda a: data_object_with_ghosts(a, CUTOFF), db))\n","print(len(data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EQKv6hfir4Ji"},"source":["y_atom = torch.tensor([d.y / d.nlocal for d in data], dtype=DTYPE)\n","y_atom_mean = y_atom.mean().item()\n","y_atom_std = y_atom.std().item()\n","print(y_atom_mean, y_atom_std)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7LU3_vTr48m"},"source":["random.shuffle(data)\n","train_amount = int(len(data) * TRAINING_RATIO)\n","train_data = data[:train_amount]\n","test_data = data[train_amount:]\n","train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ds3hZ1RSp2xq"},"source":["# Model\n"]},{"cell_type":"code","metadata":{"id":"kKkXNgk25L0J"},"source":["#@title  { vertical-output: true, form-width: \"25%\" }\n","from torch_geometric.nn import DimeNet\n","from torch_geometric.nn.acts import swish\n","from math import sqrt, pi as PI\n","\n","import numpy as np\n","import torch\n","from torch.nn import Linear, Embedding\n","from torch_scatter import scatter\n","from torch_sparse import SparseTensor\n","from torch_geometric.nn import radius_graph\n","from torch_geometric.data import download_url\n","from torch_geometric.data.makedirs import makedirs\n","\n","from torch_geometric.nn.models.dimenet import Envelope\n","from torch_geometric.nn.models.dimenet import BesselBasisLayer\n","from torch_geometric.nn.models.dimenet import SphericalBasisLayer\n","from torch_geometric.nn.models.dimenet import ResidualLayer\n","from torch_geometric.nn.models.dimenet import InteractionBlock\n","from torch_geometric.nn.models.dimenet import OutputBlock\n","\n","from torch_geometric.nn.models.dimenet_utils import bessel_basis, real_sph_harm\n","import ase\n","from ase.neighborlist import neighbor_list \n","from ase import Atoms\n","\n","from torch_geometric.data import DataLoader\n","\n","try:\n","    import sympy as sym\n","except ImportError:\n","    sym = None\n","\n","import os\n","try:\n","    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","    import tensorflow as tf\n","except ImportError:\n","    tf = None\n","\n","# TODO: move this somewhere else\n","device = DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","DTYPE = torch.float64\n","\n","\n","def pbc_edges(cutoff, z, x, cell, batch, compute_sc=False):\n","\n","  NH1 = torch.tensor([], dtype=torch.long, device=DEVICE)\n","  NH2 = torch.tensor([], dtype=torch.long, device=DEVICE)\n","  S = torch.tensor([], dtype=torch.long, device=DEVICE)\n","  D = torch.tensor([], dtype=DTYPE, device=DEVICE)\n","  SC = torch.tensor([], dtype=DTYPE, device=DEVICE) if compute_sc else None\n","  x_ = torch.clone(x).detach().cpu().numpy()\n","\n","  if batch is not None:\n","    # count number of elements for each batch\n","    batch_ids = list(set(batch.cpu().tolist()))\n","    batch_sizes = [ (batch == id).sum().item() for id in batch_ids ]\n","\n","    for i in range(len(batch_sizes)):\n","      offset = sum(batch_sizes[:i]) # to obtain correct atom indices\n","      \n","      atoms = Atoms(charges = (z[offset:offset + batch_sizes[i]]).cpu(), \n","        positions = x_[offset:offset + batch_sizes[i]], \n","        cell = (cell[3*i:3*(i+1)]).cpu(),\n","        pbc=True\n","      ) \n","      \n","      nh1, nh2, s = neighbor_list(\"ijS\", atoms, cutoff, self_interaction=False) \n","      nh1 = torch.tensor(nh1, dtype=torch.long, device=DEVICE)\n","      nh2 = torch.tensor(nh2, dtype=torch.long, device=DEVICE)\n","      nh1 = nh1 + offset\n","      nh2 = nh2 + offset\n","      s = torch.tensor(s, dtype=DTYPE, device=DEVICE)\n","      d = x[nh2] - x[nh1] + torch.matmul(s, cell[3*i:3*(i+1)])\n","      \n","      if compute_sc:\n","        cell_flat = torch.flatten(cell[3*i:3*(i+1)])\n","        sc = torch.tile(cell_flat, (len(d), 1))\n","        sc[:, 0:3] = (sc[:, 0:3].T * s[:, 0]).T\n","        sc[:, 3:6] = (sc[:, 3:6].T * s[:, 1]).T\n","        sc[:, 6:9] = (sc[:, 6:9].T * s[:, 2]).T\n","        SC = torch.cat((SC, sc), 0)      \n","\n","      NH1 = torch.cat((NH1, nh1), 0)\n","      NH2 = torch.cat((NH2, nh2), 0)\n","      S = torch.cat((S, s), 0)\n","      D = torch.cat((D, d), 0)\n","\n","  else: # no batch\n","    atoms = Atoms(charges = z.cpu(), positions = x.detach().numpy(), cell = cell.cpu(), pbc=True)\n","    nh1, nh2, s = neighbor_list(\"ijS\", atoms, cutoff, self_interaction=False)\n","    nh1 = torch.tensor(nh1, dtype=torch.long, device=DEVICE)\n","    nh2 = torch.tensor(nh2, dtype=torch.long, device=DEVICE)\n","    s = torch.tensor(s, dtype=DTYPE, device=DEVICE)\n","    d = x[nh2] - x[nh1] + torch.matmul(s, cell)\n","    \n","    if compute_sc:\n","      cell_flat = torch.flatten(cell)\n","      sc = torch.tile(cell_flat, (len(d), 1))\n","      sc[:, 0:3] = (sc[:, 0:3].T * s[:, 0]).T\n","      sc[:, 3:6] = (sc[:, 3:6].T * s[:, 1]).T\n","      sc[:, 6:9] = (sc[:, 6:9].T * s[:, 2]).T  \n","      SC = sc\n","\n","    NH1, NH2, S, D = nh1, nh2, s, d\n","\n","  D = D.norm(dim=-1)\n","  return  NH1, NH2, D, S, SC \n","\n","class EmbeddingBlock(torch.nn.Module):\n","  def __init__(self, num_radial, hidden_channels, act=swish):\n","    super(EmbeddingBlock, self).__init__()\n","    self.act = act\n","\n","    self.emb = Embedding(95, hidden_channels)\n","    self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n","    self.lin = Linear(3 * hidden_channels, hidden_channels)\n","\n","    self.reset_parameters()\n","\n","  def reset_parameters(self):\n","    self.emb.weight.data.uniform_(-sqrt(3), sqrt(3))\n","    self.lin_rbf.reset_parameters()\n","    self.lin.reset_parameters()\n","\n","  def forward(self, x, rbf, i, j):\n","    x = self.emb(x)\n","    #rbf = self.act(self.lin_rbf(rbf)) # FIX: this should not have an activation function\n","    rbf = self.lin_rbf(rbf)\n","    return self.act(self.lin(torch.cat([x[i], x[j], rbf], dim=-1)))\n","\n","\n","\n","class DimeNetx(torch.nn.Module):\n","  \n","  def __init__(self, hidden_channels, out_channels, num_blocks, num_bilinear,\n","                 num_spherical, num_radial, cutoff=5.0, envelope_exponent=5,\n","                 num_before_skip=1, num_after_skip=2, num_output_layers=3,\n","                 act=swish, mean=None, std=None):\n","    super(DimeNetx, self).__init__()\n","\n","    self.cutoff = cutoff\n","\n","    #set mean and standard deviation of energies\n","    self.mean = mean \n","    self.std = std\n","\n","    # padding used for PBCs\n","    self.padding = torch.nn.ConstantPad2d((0,6,0,0), 0)\n","\n","    if sym is None:\n","        raise ImportError('Package `sympy` could not be found.')\n","\n","    self.num_blocks = num_blocks\n","\n","    self.rbf = BesselBasisLayer(num_radial, cutoff, envelope_exponent)\n","    self.sbf = SphericalBasisLayer(num_spherical, num_radial, cutoff,\n","                                    envelope_exponent)\n","\n","    self.emb = EmbeddingBlock(num_radial, hidden_channels, act)\n","\n","    self.output_blocks = torch.nn.ModuleList([\n","        OutputBlock(num_radial, hidden_channels, out_channels,\n","                    num_output_layers, act) for _ in range(num_blocks + 1)\n","    ])\n","\n","    self.interaction_blocks = torch.nn.ModuleList([\n","        InteractionBlock(hidden_channels, num_bilinear, num_spherical,\n","                          num_radial, num_before_skip, num_after_skip, act)\n","        for _ in range(num_blocks)\n","    ])\n","\n","    self.reset_parameters()\n","\n","  def reset_parameters(self):\n","    self.rbf.reset_parameters()\n","    self.emb.reset_parameters()\n","    for out in self.output_blocks:\n","      out.reset_parameters()\n","    for interaction in self.interaction_blocks:\n","      interaction.reset_parameters()\n","\n","  def triplets_original(self, edge_index, num_nodes):\n","    row, col = edge_index  # j->i\n","\n","    value = torch.arange(row.size(0), device=row.device)\n","    adj_t = SparseTensor(row=col, col=row, value=value,\n","                         sparse_sizes=(num_nodes, num_nodes))\n","    adj_t_row = adj_t[row]\n","    num_triplets = adj_t_row.set_value(None).sum(dim=1).to(torch.long)\n","\n","    # Node indices (k->j->i) for triplets.\n","    idx_i = col.repeat_interleave(num_triplets)\n","    idx_j = row.repeat_interleave(num_triplets)\n","    idx_k = adj_t_row.storage.col()\n","    mask = (idx_i != idx_k)  # Remove i == k triplets.\n","    idx_i, idx_j, idx_k = idx_i[mask], idx_j[mask], idx_k[mask]\n","\n","    # Edge indices (k-j, j->i) for triplets.\n","    idx_kj = adj_t_row.storage.value()[mask]\n","    idx_ji = adj_t_row.storage.row()[mask]\n","\n","    return col, row, idx_i, idx_j, idx_k, idx_kj, idx_ji\n","\n","\n","  def triplets(self, edge_index, num_nodes, shift_cells=None, shift=None):\n","    row, col = edge_index  # j->i\n","\n","    value = torch.arange(row.size(0), device=row.device)\n","    adj_t = SparseTensor(row=col, col=row, value=value,\n","                         sparse_sizes=(num_nodes, num_nodes))\n","    adj_t_row = adj_t[row]\n","    num_triplets = adj_t_row.set_value(None).sum(dim=1).to(torch.long)\n","\n","    idx_i = col.repeat_interleave(num_triplets)\n","    idx_j = row.repeat_interleave(num_triplets)\n","    idx_k = adj_t_row.storage.col()\n","\n","    if shift_cells is not None: # Update also the shift vectors\n","      shift_cells_i = shift_cells.repeat_interleave(num_triplets, dim=0)\n","      shift_i = shift.repeat_interleave(num_triplets, dim=0)\n","      shift_cells_k = -shift_cells[adj_t_row.storage.value()]\n","      shift_k = -shift[adj_t_row.storage.value()]\n","\n","    mask = torch.all((torch.cat((torch.unsqueeze(idx_i, 1), shift_i), dim=1) ==\\\n","                      torch.cat((torch.unsqueeze(idx_k, 1), shift_k), dim=1)), dim=1)\n","\n","    idx_i, idx_j, idx_k = idx_i[~mask], idx_j[~mask], idx_k[~mask]\n","    if shift_cells is not None: # Remove also from the shift vector\n","      shift_cells_i = shift_cells_i[~mask]\n","      shift_cells_k = shift_cells_k[~mask]\n","      shift_i = shift_i[~mask]\n","      shift_k = shift_k[~mask]\n","\n","    idx_kj = adj_t_row.storage.value()[~mask]\n","    idx_ji = adj_t_row.storage.row()[~mask]\n","\n","    return col, row, idx_i, idx_j, idx_k, idx_kj, idx_ji, shift_cells_i, shift_i, shift_cells_k, shift_k\n","\n","\n","  def forward(self, z, pos, idx_local, cell=None, batch=None):\n","\n","    edge_index = []\n","    dist = []\n","    shift_cells = None\n","    if cell is not None: # implement PBC\n","      r1, r2, dist, shift, shift_cells = pbc_edges(self.cutoff, z, pos, cell, batch, compute_sc=True) \n","      edge_index = [r1, r2]\n","\n","        \n","      i, j, idx_i, idx_j, idx_k, idx_kj, idx_ji, shift_cells_i, shift_i, shift_cells_k, shift_k = self.triplets(\n","          edge_index, num_nodes=z.size(0), shift_cells=shift_cells, shift=shift)        \n","    else: # old method without PBC\n","      edge_index = radius_graph(pos, r=self.cutoff, batch=batch)\n","      i, j, idx_i, idx_j, idx_k, idx_kj, idx_ji = self.triplets_original(\n","          edge_index, num_nodes=z.size(0))        \n","      dist = (pos[i] - pos[j]).pow(2).sum(dim=-1).sqrt()\n","\n","    # Define atoms position \n","    pos_i = pos[idx_i]\n","    pos_j = pos[idx_j]\n","    pos_k = pos[idx_k]\n","\n","    if cell is not None: # Fix coordinates for PBCs\n","\n","      pos_i = pos_i + shift_cells_i[:, 0:3] + shift_cells_i[:, 3:6] + shift_cells_i[:, 6:9]\n","      pos_k = pos_k + shift_cells_k[:, 0:3] + shift_cells_k[:, 3:6] + shift_cells_k[:, 6:9]\n","      sc_ij = torch.all(~torch.all(pos_i == pos_j, dim=1)) \n","      sc_kj = torch.all(~torch.all(pos_k == pos_j, dim=1))\n","      #if not (sc_ij and sc_kj):\n","      #   raise NameError('Found same position for different atoms!')\n","\n","    # Calculate angles - with some Fixes to indexes compared to the orig. version\n","    pos_ji, pos_kj = pos_j - pos_i, pos_k - pos_j\n","    a = (pos_ji * pos_kj).sum(dim=-1)\n","    b = torch.cross(pos_ji, pos_kj).norm(dim=-1) \n","    angle = torch.atan2(b, a)      \n","\n","    rbf = self.rbf(dist)\n","    sbf = self.sbf(dist, angle, idx_kj)\n","\n","    # Embedding block.\n","    x = self.emb(z, rbf, i, j)\n","    P = self.output_blocks[0](x, rbf, i, num_nodes=pos.size(0))\n","\n","    # Interaction blocks.\n","    for interaction_block, output_block in zip(self.interaction_blocks,\n","                                               self.output_blocks[1:]):\n","      x = interaction_block(x, rbf, sbf, idx_kj, idx_ji)\n","      a = output_block(x, rbf, i, num_nodes=pos.size(0))\n","      P += a\n","\n","    # Energy de-standardization\n","    if self.std is not None and self.mean is not None:\n","      P = P * self.std + self.mean\n","    \n","    P = P[idx_local]\n","    res = P.sum(dim=0) if batch is None else scatter(P, batch[idx_local], dim=0)\n","    return res"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DQvcjWezqZof"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"NiQx-JJotPPz"},"source":["## Define functions\n"]},{"cell_type":"code","metadata":{"id":"R90-DNYT8Wfu"},"source":["import gc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvEPZD8Aqb_m"},"source":["def energy_loss(y, p_energies):\n","  energies_loss = torch.mean(torch.abs(y - p_energies))\n","  return energies_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Qub6vw-qeg1"},"source":["def energy_forces_loss(y, p_energies, p_forces, energy_coeff):\n","  energies_loss = torch.mean(torch.abs(y - p_energies))\n","  forces_loss = torch.mean(torch.abs(data.f - p_forces))\n","  total_loss = (energy_coeff)*energies_loss + (1-energy_coeff)*forces_loss\n","  return total_loss, energies_loss, forces_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HH1gE7FqgdD"},"source":["def train(model, loader, optimizer, use_forces=False, energy_coeff=None):\n","  if not use_forces:\n","    train_energy(model, loader, optimizer)\n","  else:\n","    train_energy_forces(model, loader, optimizer, energy_coeff)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpdD5Z3YqiTg"},"source":["def train_energy(model, loader, optimizer):\n","  model.train()\n","  for i, data in enumerate(loader):\n","    optimizer.zero_grad()\n","    z, x, batch, y = data.z.to(DEVICE), data.x.to(DEVICE), data.batch.to(DEVICE), data.y.to(DEVICE)\n","    acc = 0\n","    batched_idx_local = torch.tensor([], dtype=torch.long)\n","    for i in range(data.num_graphs):\n","      ex = data.get_example(i)\n","      batched_idx_local = torch.cat((batched_idx_local, ex.idx_local + acc)) \n","      acc += ex.nlocal + ex.nghost\n","    batched_idx_local = batched_idx_local.to(DEVICE)\n","    e = model(z, x, batched_idx_local, cell=None, batch=batch)\n","    e = e.squeeze(1) \n","    e_loss = energy_loss(y, e)\n","\n","    e_loss.backward()\n","    optimizer.step()\n","    del z\n","    del x\n","    del batch\n","    del y\n","    del batched_idx_local\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zz9tRBdaql23"},"source":["def train_energy_forces(model, loader, optimizer, energy_coeff):\n","  model.train()\n","  total_e_loss = 0\n","  total_f_loss = 0\n","  total_ef_loss = 0\n","\n","  for data in loader:\n","    z, x, batch, y = data.z.to(DEVICE), data.x.to(DEVICE), data.batch.to(DEVICE), data.y.to(DEVICE)\n","    x.requires_grad = True \n","    optimizer.zero_grad()\n","    \n","    e = model(z, x, cell=None, batch=batch)\n","    f = -1 * torch.autograd.grad(e, x, grad_outputs=torch.ones_like(e), create_graph=True, retain_graph=True)[0]\n","    e = e.squeeze(1) \n","    \n","    ef_loss, e_loss, f_loss = energy_forces_loss(data, e, f, energy_coeff)\n","    with torch.no_grad():\n","      total_e_loss += e_loss.item()\n","      total_f_loss += f_loss.item()\n","      total_ef_loss += ef_loss.item()\n","\n","    ef_loss.backward()\n","    optimizer.step()\n","    del z\n","    del x\n","    del batch\n","    del y\n","    del batched_idx_local\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","  \n","  print(\"Total training loss\\t ef: {}, e: {}, f: {}\".format(total_ef_loss, total_e_loss, total_f_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOYz8Q0UtWIN"},"source":["def test(model, train_loader, test_loader):\n","  model.eval()\n","\n","  with torch.no_grad():\n","    train_errs = torch.tensor([], dtype=DTYPE)\n","    test_errs = torch.tensor([], dtype=DTYPE)\n","  \n","    for data in train_loader:  # Iterate in batches over the training/test dataset.\n","      z, x, batch = data.z.to(DEVICE), data.x.to(DEVICE), data.batch.to(DEVICE)\n","      acc = 0\n","      batched_idx_local = torch.tensor([], dtype=torch.long)\n","      for i in range(data.num_graphs):\n","        ex = data.get_example(i)\n","        batched_idx_local = torch.cat((batched_idx_local, ex.idx_local + acc)) \n","        acc += ex.nlocal + ex.nghost\n","      batched_idx_local = batched_idx_local.to(DEVICE)\n","      e = model(z, x, batched_idx_local, cell=None, batch=batch)\n","      e = e.squeeze(1)\n","      errs = torch.abs(e.view(-1).cpu() - data.y.cpu())\n","      train_errs = torch.cat((train_errs, errs))\n","      torch.cuda.empty_cache()\n","      del z\n","      del x\n","      del batch\n","      del batched_idx_local\n","      gc.collect()\n","    train_mae = torch.mean(train_errs).item()\n","\n","    for data in test_loader:  # Iterate in batches over the training/test dataset.\n","      z, x, batch = data.z.to(DEVICE), data.x.to(DEVICE), data.batch.to(DEVICE)\n","      acc = 0\n","      batched_idx_local = torch.tensor([], dtype=torch.long)\n","      for i in range(data.num_graphs):\n","        ex = data.get_example(i)\n","        batched_idx_local = torch.cat((batched_idx_local, ex.idx_local + acc)) \n","        acc += ex.nlocal + ex.nghost\n","      batched_idx_local = batched_idx_local.to(DEVICE)\n","      e = model(z, x, batched_idx_local, cell=None, batch=batch)\n","      e = e.squeeze(1)\n","      errs = torch.abs(e.view(-1).cpu() - data.y.cpu())\n","      test_errs = torch.cat((test_errs, errs))\n","      torch.cuda.empty_cache()\n","      del z\n","      del x\n","      del batch\n","      del batched_idx_local\n","      gc.collect()\n","    test_mae = torch.mean(test_errs).item()\n","  \n","  return train_mae, test_mae"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2gQAclirtXTC"},"source":["from datetime import datetime\n","\n","train_maes = []\n","test_maes = []\n","def train_and_test(model, train_loader, test_loader, optimizer, scheduler, \n","                   use_forces=False, energy_coeff=None, \n","                   epochs=100, starting_epoch=1, save_every=5, \n","                   name=None, description=None):\n","  if name == None:\n","    name = \"SchNet\"\n","  if description == None:\n","    description = datetime.now().strftime('%H:%M:%S')\n","\n","  for epoch in range(starting_epoch, epochs):\n","    print(\"\")\n","    print(f\"Epoch {epoch} ({datetime.now().strftime('%H:%M:%S')})\")\n","    print(f\"\")\n","    train(model, train_loader, optimizer, use_forces=use_forces, energy_coeff=energy_coeff)\n","    train_mae, test_mae = test(model, train_loader, test_loader)\n","    train_maes.append(train_mae)\n","    test_maes.append(test_mae)\n","    scheduler.step(test_mae)\n","    print(f\"Energy MAE: train {train_mae} eV, test {test_mae} eV\")\n","    if epoch % save_every == 0:\n","      save_model(name=name, description=description, epoch=epoch, train_err=train_mae, test_err=test_mae)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R34A6noCtYx5"},"source":["def save_model(name, description, epoch=-1, train_err=-1, test_err=-1):\n","  model_data = {\n","    \"desc\": \"DimeNet external ghosts\",\n","    \"str\": model_str,\n","    \"mean\": y_atom_mean,\n","    \"std\": y_atom_std,\n","    \"state\": model.state_dict(),\n","    \"optimizer\": optimizer.state_dict(),\n","    \"scheduler\": scheduler.state_dict(),\n","    \"train_err\": train_err,\n","    \"test_err\": test_err\n","  }  \n","  if not epoch == -1:\n","    epoch_str = \"_\" + str(epoch)\n","  else:\n","    epoch_str = \"\"\n","  actual_name = f\"{name}{epoch_str}\"\n","\n","  dir = f\"{MODELS_PATH}/{name}\"\n","  if not os.path.exists(dir):\n","    os.makedirs(dir)\n","    print(f\"Created directory {dir}\")\n","  torch.save(model_data, f\"{dir}/{actual_name}\")\n","  print(f\"Saved {actual_name}\")\n","\n","  if os.path.isfile(f\"{dir}/{name}_best\"):\n","    best = torch.load(f\"{dir}/{name}_best\")\n","    best_train_err = best[\"train_err\"]\n","    best_test_err = best[\"test_err\"]\n","  else:\n","    best_train_err, best_test_err = 10e18, 10e18\n","  if test_err < best_test_err:\n","    torch.save(model_data, f\"{dir}/{name}_best\")\n","    print(f\"Saved best\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usAV1Y-Qth72"},"source":["## Instantiate model"]},{"cell_type":"code","metadata":{"id":"4jgbJMqotifV"},"source":["model_name = \"dimenet_35_extghost3\" # name of the folder where checkpoints will be saved\n","model_description = \"DimeNet with external ghosts, batched idx local masking\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iy8vyhFitjda"},"source":["model = DimeNetx(hidden_channels=128, out_channels=1, num_blocks=7, num_bilinear=8, num_spherical=7, num_radial=6, cutoff=3.5, std=y_atom_std, mean=y_atom_mean).to(device)\n","\n","if DTYPE == torch.float64:\n","  model = model.double()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","optimizer.zero_grad()\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='abs')\n","use_forces = False\n","energy_coeff = 0.\n","max_epochs = 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B5Lz3enDtluj"},"source":["Copy-paste the content of the previous cell in the following variable to allow for saving of parameters + hyperparameters"]},{"cell_type":"code","metadata":{"id":"-hA3uobYtml1"},"source":["model_str = \"\"\"\n","model = DimeNetx(hidden_channels=128, out_channels=1, num_blocks=7, num_bilinear=8, num_spherical=7, num_radial=6, cutoff=3.5, std=y_atom_std, mean=y_atom_mean).to(device)\n","\n","if DTYPE == torch.float64:\n","  model = model.double()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","optimizer.zero_grad()\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='abs')\n","use_forces = False\n","energy_coeff = 0.\n","max_epochs = 100\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kxKQFWV_toOq"},"source":["# [Restore from checkpoint]"]},{"cell_type":"code","metadata":{"id":"xnLQn-FbtpK7"},"source":["model_fn = \"dimenet_35_extghost3_45\"\n","checkpoint = torch.load(MODELS_PATH + \"/dimenet_35_extghost3/\" + model_fn)\n","model.load_state_dict(checkpoint[\"state\"])\n","optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","scheduler.load_state_dict(checkpoint[\"scheduler\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wSXWWjl0qqIf"},"source":["# Info"]},{"cell_type":"code","metadata":{"id":"3eXmO92Jtrkl"},"source":["#@title  { vertical-output: true, display-mode: \"both\" }\n","def experiment_summary():\n","  device = \"CPU\" if DEVICE == torch.device(\"cpu\") else f\"GPU ({torch.cuda.get_device_name(device=DEVICE)})\"\n","  print(f\"Device: {device}\")\n","  print(f\"Data type: {DTYPE}\")\n","  print(f\"Files used: {file_names}\")\n","  print(f\"Cutoff: {3.5}\")\n","  print(f\"Mean: {y_atom_mean}, std: {y_atom_std}\")\n","  print(f\"Training ratio: {TRAINING_RATIO}\")\n","  print(f\"Batch size: {BATCH_SIZE}\")\n","  print(f\"Model: \\n    {model_name}: {model_description}\\n    {str(model)}\")\n","  loss = \"energy\" if not use_forces else f\"energy+forces ({energy_coeff})\"\n","  print(f\"Loss: {loss}\")\n","  print(f\"Optimizer: {optimizer}\")\n","  print(f\"Scheduler: {scheduler}\")\n","  print(f\"Max epochs: {max_epochs}\")\n","experiment_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKqnNmmi-FuT"},"source":["torch.cuda.get_device_name(device=DEVICE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRxUXTjY5cO1"},"source":["#@title  { vertical-output: true, display-mode: \"both\" }\n","from psutil import *\n","print(cpu_count())\n","print(cpu_stats())\n","!cat /proc/cpuinfo\n","!df -h\n","print(virtual_memory())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-G4PXuDW9Hgt"},"source":["train_and_test(model, train_loader, test_loader, optimizer, scheduler, \n","               use_forces=use_forces, energy_coeff=energy_coeff,\n","               epochs=max_epochs, starting_epoch=46, save_every=1,\n","               name=model_name, description=model_description)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APtIHa1UrGEx"},"source":["## [Clean memory]"]},{"cell_type":"code","metadata":{"id":"tPyeH_HJrLau"},"source":["import gc\n","if model:\n","  model.cpu()\n","  del model\n","  gc.collect()\n","  torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]}]}